---
layout: distill
title: "&#x1f4e2; KaoGPT: Studying the Performance of Text Generating Models"
img: /assets/img/kaogpt.png
date: 2023-03-22
tags: NLP, generative_AI, LLM
categories: research

authors:
    - name: Kuan Heng (Jordan) Lin
      url: "https://kuanhenglin.github.io"
      affiliations:
        name: UCLA CS
    - name: Margaret Capetz
      url: "https://www.linkedin.com/in/margaret-capetz"
      affiliations:
        name: UCLA CSE
    - name: Jeffrey Kwan
      url: "https://hametar0u.github.io/"
      affiliations:
        name: UCLA CS
    - name: Prateik Sinha
      url: "https://in.linkedin.com/in/prateik-sinha-644918202"
      affiliations:
        name: "UCLA MoC + Stats"

---

> We developed text-generation models, including the RNN, decoder stack, encoder-decoder, and fine-tuned GPT-2, to emulate Professor Kaoâ€™s lectures. Through experimentation, we found that finetuning GPT-2 led to a model that outperformed all others. However, given the limited dataset, the trained-from-scratch decoder stack performed surprisingly well. Our results offer insights into the strengths and limitations of various text generation models, aiding researchers in selecting the most suitable model for their needs.

<iframe class="iframe-pdf" src="/assets/pdf/kaogpt.pdf" frameborder="0"></iframe>

This paper was written for the final project of UCLA's ECE C147: Neural Networks and Deep Learning, Winter 2023, taught by Professor [Jonathan Kao](http://seas.ucla.edu/~kao/).