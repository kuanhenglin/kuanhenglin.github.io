<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kuanhenglin.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kuanhenglin.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-30T17:44:14+00:00</updated><id>https://kuanhenglin.github.io/feed.xml</id><title type="html">blank</title><subtitle>Kuan Heng (Jordan) Lin | CS + Math @ UCLA | Student Researcher @ Zhou Lab | URC-Sciences Summer | LA Program PDT | ACM AI </subtitle><entry><title type="html">ü§ù people2vec: LA Hacks 2023 3rd Prize Overall</title><link href="https://kuanhenglin.github.io/blog/2023/people2vec/" rel="alternate" type="text/html" title="ü§ù people2vec: LA Hacks 2023 3rd Prize Overall"/><published>2023-04-23T00:00:00+00:00</published><updated>2023-04-23T00:00:00+00:00</updated><id>https://kuanhenglin.github.io/blog/2023/people2vec</id><content type="html" xml:base="https://kuanhenglin.github.io/blog/2023/people2vec/"><![CDATA[<blockquote> <p>Awarded <strong>3rd Place Overall</strong> as an entry for <a href="https://devpost.com/software/people2vec"><strong>LA Hacks 2023</strong></a>. We use users‚Äô YouTube watch history and match them with those with similar media interests via LLM and large CV model embeddings. Particularly, we visualize user interest overlap (and divergence) with <strong>video clouds</strong> (embedding PCA point clouds) to reveal similar interests while preserving privacy (i.e., without revealing the other person‚Äôs YouTube histories).</p> </blockquote> <div class="repo p-2 text-center github-repo-in-post"> <a href="https://github.com/hametar0u/people2vec" rel="external nofollow noopener" target="_blank"> <img class="repo-img-light w-100" alt="hametar0u/people2vec" src="https://github-readme-stats.vercel.app/api/pin/?username=hametar0u&amp;repo=people2vec&amp;theme=default&amp;show_owner=true"/> <img class="repo-img-dark w-100" alt="hametar0u/people2vec" src="https://github-readme-stats.vercel.app/api/pin/?username=hametar0u&amp;repo=people2vec&amp;theme=dark&amp;show_owner=true"/> </a> </div> <p><img src="/assets/img/people2vec/people2vec_1.png" width="100%"/></p> <p>Think of the last time you made a meaningful connection. What were you talking about? That one really funny youtube video from years ago? The new marvel movie trailer? Some deep-dive analysis into a topic that really resonated with you? Nowadays, our interests and identities are often reflected in the content we consume. Yet it is not something we actually think about when forming new connections.</p> <p>Many of us are in our comfort zone with our friend groups in college. But what about when we graduate, go to study abroad, or have to move somewhere for a new job? When you try to meet new people and talk to them for the first time, what‚Äôs your icebreaker going to be?</p> <p>That‚Äôs why we made people2vec. A web app that matches you with people of similar interests, based on your YouTube watch histories.</p> <h2 id="inspiration">Inspiration</h2> <p>The lot of us are all machine learning (ML) people, having worked with both natural language processing (NLP) and computer vision (CV) problems before, and we understand the importance of embeddings, i.e., vector representations of semantic information (e.g., text and images) that can be very helpful.</p> <p>It seems like the only way to meet people nowadays is via dating apps, some based on looks, some based on ‚Äúpersonality,‚Äù etc. However, oftentimes what bonds people together is their shared interests in media consumption, and one of the most popular platforms for doing so is YouTube. Why not recreate the excitement and fun of sharing your new favorite YouTube video with your friends, with people who you otherwise may not have met?</p> <p>The name people2vec makes an ohmage to word2vec, the very original popular text embedding technique that kickstarted it all.</p> <h2 id="what-it-does">What it does</h2> <p><img src="/assets/img/people2vec/people2vec_4.png" width="100%"/></p> <p>People2vec finds people with similar interests based on your social media activity. In particular, we use users‚Äô YouTube watch history and match them with those with similar interests via embeddings from both NLP and CV methods. Additionally, we provide interesting visualizations that intuitively display how your YouTube interests match and differ with others.</p> <h2 id="how-we-built-it">How we built it</h2> <p><img src="/assets/img/people2vec/people2vec_3.png" width="100%"/></p> <p>The people2vec tech stack consists of a Node.js and React.js frontend and Django backend. The NLP pipeline uses Cohere‚Äôs amazing API of their multilingual model (due to YouTube video titles having multiple languages) to obtain embeddings of video titles, and the CV pipeline uses PyTorch‚Äôs InceptionV3 with weights pretrained on ImageNet1K and the last classification layer set to identity. Particularly, we take inspiration from the Fr√©chet Inception Distance (FID) from generative CV, where we compute the similarity between two user‚Äôs YouTube video interests by comparing the distribution of their respective watch history videos‚Äô title and thumbnail embeddings.</p> <p>We embed TensorBoard‚Äôs PCA to visualize video title embeddings between two users, and we use YouTube‚Äôs API to fetch thumbnails. We also use Deck.GL and Google Maps‚Äôs Geocoding API for location-based information.</p> <h2 id="challenges-we-ran-into">Challenges we ran into</h2> <p><img src="/assets/img/people2vec/people2vec_2.png" width="100%"/></p> <p>One of the main challenges we ran into is API-related, particularly related to YouTube. We wanted to have a way to directly fetch watch history (and other analytics information of a user) by asking them to authenticate their Google account with us, but we quickly discovered that doing so is simply impossible, hence the relatively jank way of asking people to upload their Google Takeout data downloads.</p> <p>Another challenge we ran into was linking everything together. We worked in quite compartmentalized ways in the beginning. Angeline developed the frontend, Jordan wrote the ML methods, Prateik handled the data parsing and processing, and Jeffrey built the backend. Each component worked well in their respective unit tests, but combining them soon proved to be a big challenge as there were many moving parts that relied on each other to be properly tested. We were, quite literally, assembling a plane as it was flying.</p> <h2 id="accomplishments-that-were-proud-of">Accomplishments that we‚Äôre proud of</h2> <p>Jordan: Using embeddings and distribution comparison methods (FID scores) worked better than I expected, especially since I am combining many different APIs and file formats into flexible modules that allowed for easy and relatively efficient computation of a similarity measure between two embedding distributions.</p> <p>Angeline: The design &lt;3</p> <p>Prateik: I am quite proud of getting the TensorBoard to work. The visualization ended up being way cooler looking than we first expected, and the path to getting it to work (hacking GitHub) was also quite arduous yet rewarding. Also, I parsed HTML with Regex. Take that computer scientists!</p> <p>Jeffrey: I‚Äôm proud of not giving up and all the friends we made along the way (zero) :heart:</p> <h2 id="what-we-learned">What we learned</h2> <p>We got to know GitHub way better. We learned the importance of a solid plan that still allows for flexibility (which we are still working on improving!). We grasped the difficulty of integrating machine learning models to full-stack applications and got better at knowing how to assemble these differing modules together. Most importantly, we learned how to work as a team &lt;3</p> <h2 id="whats-next-for-people2vec">What‚Äôs next for people2vec</h2> <p>We think a great next direction is security related. Particularly, it is very possible to borrow aspects of federated learning to better preserve user privacy (especially with information like YouTube watch history). For example, we can do pre-processing (e.g., first-layer pass through the neural network) on the frontend (i.e., ‚Äúlocally,‚Äù or on the user‚Äôs device) to obtain some initial embedding that greatly obfuscates user data and complete the rest of the processing in the backend. This ensures that we can still extract information about the user‚Äôs YouTube video interests while not directly having a copy of their information.</p>]]></content><author><name>Jordan Lin</name></author><category term="project"/><category term="NLP,"/><category term="social_media,"/><category term="LLM"/><summary type="html"><![CDATA[Awarded 3rd Place Overall as an entry for LA Hacks 2023. We use users‚Äô YouTube watch history and match them with those with similar media interests via LLM and large CV model embeddings. Particularly, we visualize user interest overlap (and divergence) with video clouds (embedding PCA point clouds) to reveal similar interests while preserving privacy (i.e., without revealing the other person‚Äôs YouTube histories).]]></summary></entry><entry><title type="html">üîÅ DDIM Inversion and Latent Space Manipulation</title><link href="https://kuanhenglin.github.io/blog/2023/ddim_inversion/" rel="alternate" type="text/html" title="üîÅ DDIM Inversion and Latent Space Manipulation"/><published>2023-03-27T00:00:00+00:00</published><updated>2023-03-27T00:00:00+00:00</updated><id>https://kuanhenglin.github.io/blog/2023/ddim_inversion</id><content type="html" xml:base="https://kuanhenglin.github.io/blog/2023/ddim_inversion/"><![CDATA[<blockquote> <p>We explore the inversion and latent space manipulation of diffusion models, particularly the denoising diffusion implicit model (DDIM), a variant of the denoising diffusion probabilistic model (DDPM) with deterministic (and acceleratable) sampling and thus a meaningful mapping from the latent space \(\mathcal{Z}\) to the image space \(\mathcal{X}\). We implement and compare optimization-based, learning-based, and hybrid inversion methods adapted from GAN inversion, and we find that optimization-based methods work well, but learning-based and hybrid methods run into obstacles fundamental to diffusion models. We also perform latent space interpolation to show that the DDIM latent space is continuous and meaningful, just like that of GANs. Lastly, we apply GAN semantic feature editing methods to DDIMs, visualizing binary attribute decision boundaries to showcase the unique interpretability of the diffusion latent space.</p> </blockquote> <div class="repo p-2 text-center github-repo-in-post" style="margin-bottom: 0em !important;"> <a href="https://github.com/kuanhenglin/ddim-inversion" rel="external nofollow noopener" target="_blank"> <img class="repo-img-light w-100" alt="kuanhenglin/ddim-inversion" src="https://github-readme-stats.vercel.app/api/pin/?username=kuanhenglin&amp;repo=ddim-inversion&amp;theme=default&amp;show_owner=false"/> <img class="repo-img-dark w-100" alt="kuanhenglin/ddim-inversion" src="https://github-readme-stats.vercel.app/api/pin/?username=kuanhenglin&amp;repo=ddim-inversion&amp;theme=dark&amp;show_owner=false"/> </a> </div> <h2 id="introduction">Introduction</h2> <p>Latent spaces are a vital part of modern generative neural networks. Existing methods such as generative adversarial networks (GANs) and variational autoencoders (VAEs) all generate high-dimensional images from some low-dimensional latent space which encodes the features of the generated images. Thus, one can sample‚Äîrandomly or via interpolation‚Äîthis latent space to generate new images, and with the case of generative adversarial networks (GAN), at relatively high fidelity.</p> <p>Since the latent space is, in a way, a low-dimensional representation of the generated images, we can model the generative network as a bijective function \(G : \mathcal{Z} \rightarrow \mathcal{X}\), where \(\mathcal{Z} \subseteq \mathbb{R}^d\) is the latent space and \(\mathcal{X} \subseteq \mathbb{R}^n\) is the image space, \(d \ll n\). Since the latent space encodes the most important visual features of the output images, to manipulate existing images, we can try to invert \(G\), \(G^{-1} : \mathcal{X} \rightarrow \mathcal{Z}\), to go from image space to latent space.</p> <p>For VAEs, \(G^{-1}\) is trivially part of the architecture, but this is not the case for GANs and diffusion models. Luckily, GAN inversion is a very well-researched field. Although finding an analytical solution to \(G^{-1}\) is difficult, there are many ways to approximate the process, including</p> <ol> <li>optimization-based methods, where we perform optimization to find the latent vector which best reconstructs the target image,</li> <li>learning-based methods, where we train encoders that approximate \(G^{-1}\), and</li> <li>hybrid methods, where we combine the two methods above, e.g., use learning-based methods to find a good initialization for optimization-based methods [2].</li> </ol> <p>The logical next step is to directly apply the above methods directly to diffusion models‚Äîbut there is a catch.</p> <h3 id="the-problem-with-diffusion-models">The Problem With Diffusion Models</h3> <p>The denoising diffusion probabilistic model (DDPM) is a relatively recent yet incredibly influential advancement in generative neural networks. Particularly, given an input image \(\mathbf{x}_0\) and some noise schedule \(\alpha_0, \alpha_1, \dots, \alpha_t\) (e.g., linear schedule) and \(\bar{\alpha}_t = \prod_{i = 1}^t \alpha_i\), it iteratively adds noise to the input image with increasing timesteps \(t \in \{ 1, 2, \dots, T \}\), described by</p> \[\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon} ,\] <p>where \(\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})\) and \(T\) is the maximum timestep chosen so that \(\mathbf{x}_T\) resembles pure noise. Then, given some timestep \(t\), the network predicts the noise that was added from \(t - 1\) to \(t\). Formally, given the model \(\boldsymbol{\epsilon}_{\theta}\), we minimize</p> \[\mathcal{L}(\theta) = \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta (\mathbf{x}_t, t) \right\|^2 = \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta \left( \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}, t \right) \right\|^2\] <p>for some uniformly sampled time step \(t \sim \mathcal{U}(\{ 1, \dots, T \})\). Notice that, even though we heuristically add noise at every time \(t\), we can reparametrize that process as a single noise-adding step from \(\mathbf{x}_0\) to \(\mathbf{x}_t\), hence the use of \(\bar{\alpha}_t\) instead of \(\alpha_t\). To avoid effectively adding the same, unchanging noise \(\boldsymbol{\epsilon}\) at every time step, during sampling, we add small amounts of random noise every time the model subtracts the predicted noise from \(\mathbf{x}_t\) to get \(\mathbf{x}_{t - 1}\). Formally, starting with \(\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})\), where \(\mathbf{x}_T \in \mathcal{Z}\), for \(t = T, \dots, 1\), we sample some \(\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})\) every iteration if \(t &gt; 1\), and we have</p> \[\mathbf{x}_{t - 1} = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta (\mathbf{x}_t, t) \right) + \sigma_t \boldsymbol{\epsilon} ,\] <p>where \(\sigma_t^2 = 1 - \alpha_t\), and we arrive at \(\mathbf{x}_0\), the generated image via denoising. Since we obtain \(\mathbf{x}_{t - 1}\) solely from \(\mathbf{x}_t\), the sampling process is a Markov chain.</p> <p>Note that, unlike GANs, for diffusion models the latent space has the same dimensions as the image space, that is, \(\mathcal{Z}, \mathcal{X} \subseteq \mathbb{R}^n\).</p> <p>The results of DDPMs are on-par with (and nowadays exceeding) GANs, and it has much greater stability in training as the generator is not trained via adversarial means [3] and thus is less prone to problems such as modal collapse, though it has the downside of a much longer sampling/generation time as it must make \(T\) forward passes through the model.</p> <p>Naturally, we ask the question: can we invert DDPMs just like we can with GANs? In order for the above inversion methods to be applied to generative networks, we need two assumptions</p> <ol> <li>the latent space maps to meaningful image features, and</li> <li>the generator, i.e., \(G\), is deterministic.</li> </ol> <p>Turns out, DDPMs does not satisfy the second assumption. Since the sampling process of DDPMs includes applying noise to the predicted \(\mathbf{x}_{t - 1}\) given \(\mathbf{x}_t\) for \(t &gt; 0\), the generation process is not deterministic. Even if \(\mathcal{Z}\) does meaningfully map to \(\mathcal{X}\), latent space manipulation (without conditioning \(\boldsymbol{\epsilon}_\theta\), i.e., without providing it some additional semantic latent vector) cannot occur when the output image changes between different sampling passes, that is, \(G(\mathbf{z})\) is changes every execution even if \(\mathbf{z} \in \mathcal{Z}\) remains constant.</p> <p>Moreover, optimization-based methods are computationally impractical for DDPMs, as since we are optimizing the reconstruction error of the generated image (compared to the target image) with respect to the (randomly initialized) latent vector, we must backpropagate through the entire Markovian sampling process with \(T\) iterations. (The standard value is \(T = 1000\), which is huge.) Not only is doing so very expensive, problems with extremely deep neural networks, such as gradient vanishing or explosion, begin to emerge.</p> <h3 id="denoising-diffusion-implicit-models">Denoising Diffusion Implicit Models</h3> <p><img src="/assets/img/ddim_inversion/non_markovian.png" width="100%"/></p> <p><em>Figure 1: Illustrated comparison between diffusion (left) and non-Markovian (right) inference models. Crucially, we predict \(\mathbf{x}_{t - 1}\) with both \(\mathbf{x}_t\) and (predicted) \(\mathbf{x}_0\).</em></p> <p>The denoising diffusion implicit model (DDIM) is a variation of the DDIM that, fundamentally, only modifies the inference/sampling step by making it non-Markovian. (In fact, Song et al. (2021) stated that pretrained DDPM models can even be used directly.) Particularly, it</p> <ol> <li>has a deterministic sampling process, and</li> <li>can accelerate sampling by taking timestep jumps larger than \(1\).</li> </ol> <p>To do so, for notational simplicity, we rewrite \(\bar{\alpha}\) as \(\alpha\). Then, recall that we are optimizing</p> \[\mathcal{L}(\theta) = \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta (\mathbf{x}_t, t) \right\|^2 = \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta \left( \sqrt{\alpha_t} \mathbf{x}_0 + \sqrt{1 - \alpha_t} \boldsymbol{\epsilon}, t \right) \right\|^2 .\] <p>Notice that \(\boldsymbol{\epsilon}_\theta (\mathbf{x}_t, t)\) is not necessarily predicting the noise we added from \(\mathbf{x}_{t - 1}\) to \(\mathbf{x}_t\) but instead, due to the reparametrization to add the noise \(\boldsymbol{\epsilon}\) in one step, the noise we added from \(\mathbf{x}_0\)‚Äîthe target image‚Äîto \(\mathbf{x}_t\). In other words, \(\boldsymbol{\epsilon}_\theta\) is separating the added noise \(\boldsymbol{\epsilon}\) from the original image \(\mathbf{x}_0 = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \sqrt{1 - \alpha_t} \boldsymbol{\epsilon} \right)\). We can also intuitively imagine how, the larger the \(t\), the less accurate the predicted target image</p> \[\hat{\mathbf{x}}_0 = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \sqrt{1 - \alpha_t} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right)\] <p>would be. Thus, during the inference process, given some \(\mathbf{x}_t\), instead of subtracting the predicted \(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\) from \(\mathbf{x}_t\) to obtain \(\mathbf{x}_{t - 1}\), we can instead reconstruct \(\mathbf{x}_{t - 1}\) from the predicted \(\mathbf{x}_0\), \(\hat{\mathbf{x}}_0\), and the predicted noise \(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\) both scaled with \(\alpha_t\)‚Äîlike during training. Formally, accounting for the random noise \(\sigma_t \boldsymbol{\epsilon}\) we add in DDPMs, we have</p> \[\mathbf{x}_{t - 1} = \sqrt{\alpha_{t - 1}} \underbrace{\left( \frac{\mathbf{x}_t - \sqrt{1 - \alpha_t} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{\alpha_t}} \right)}_{\textrm{predicted } \mathbf{x}_0 \textrm{, i.e., } \hat{\mathbf{x}}_0} + \sqrt{1 - \alpha_{t - 1} - \sigma_t^2} \underbrace{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}_{\textrm{added noise}} + \underbrace{\sigma_t \boldsymbol{\epsilon}}_{\textrm{random noise}} ,\] <p>where we define \(\alpha_0 = 1\). Crucially, different choices of \(\sigma_t\) results in different generative processes despite the training objective and resulting model \(\boldsymbol{\epsilon}_\theta\) remaining the same. Particularly, when \(\sigma_t = \sqrt{\frac{1 - \alpha_{t - 1}}{1 - \alpha_t}} \sqrt{1 - \frac{\alpha_t}{\alpha_{t - 1}}}\) (in this more complex form because we rewrote \(\bar{\alpha}\) as \(\alpha\)), the process is Markovian and becomes equivalent to the DDPM. On the other hand, when \(\sigma_t = 0\) for all \(t \in \{ 1, \dots, T \}\), the sampling process becomes deterministic as we are no longer adding random noise every iteration. Therefore, as Song et al. (2021) claims, the model becomes an implicit probabilistic model, thus the name <em>denoising diffusion implicit model</em> (DDIM). Since now we are mirroring the forward process in the sampling process, where \(\mathbf{x}_t\) could depend on both \(\mathbf{x}_0\) and \(\mathbf{x}_{t - 1}\), the inference step is non-Markovian.</p> <p>Moreover, since we have the predicted \(\mathbf{x}_0\) and \(\boldsymbol{\epsilon}\), we can use them to reconstruct <em>any</em> timestep of \(\mathbf{x}\). Particularly, we can write</p> \[\mathbf{x}_{t - \Delta t} = \sqrt{\alpha_{t - \Delta t}} \left( \frac{\mathbf{x}_t - \sqrt{1 - \alpha_t} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{\alpha_t}} \right) + \sqrt{1 - \alpha_{t - \Delta t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\] <p>for any \(\Delta t \leq t\), where we can drastically accelerate the inference process by setting \(\Delta t \gg 1\). Doing so makes optimization-based inversion methods possible for large \(\Delta t\). In practice, DDIMs tend to have slightly worse generation quality than DDPMs, especially with \(\Delta t \gg 1\), mostly in the decrease in texture and detail [1].</p> <p>In summary, DDIMs address the problems with DDPMs by making the sampling process deterministic and fast. Therefore, with that foundation, we aim to apply GAN inversion methods to DDIMs and explore if we can reverse the diffusion process.</p> <h2 id="implementing-ddims">Implementing DDIMs</h2> <p>We closely follow the original PyTorch implementation of the DDIM [1] with some slight modifications to account for the much reduced network sizes (due to computational limits) inspired by [2]. All models are trained and tuned from scratch as the pretrained models are generally too large for optimization-based methods.</p> <p>Notably, <em>we will be training everything from scratch</em> and not using any pre-trained models. Many diffusion-based generative models today are incredibly massive and impractical to train and test on single computers, so we also want to explore training smaller networks.</p> <h3 id="data">Data</h3> <p>We train DDIMs on several datasets, including</p> <ul> <li><a href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a>: <em>Large-scale CelebFaces Attributes Dataset</em>,</li> <li><a href="https://github.com/NVlabs/ffhq-dataset">FFHQ</a>: <em>Flickr-Faces-HQ Dataset</em>,</li> <li><a href="https://www.yf.io/p/lsun">LSUN Churches</a>: <em>Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop</em>, <code class="language-plaintext highlighter-rouge">church_outdoor</code> class,</li> <li><a href="https://www.robots.ox.ac.uk/~vgg/data/flowers/102/">Flowers102</a>: <em>102 Category Flower Dataset</em>,</li> <li><a href="https://image-net.org/download-images.php">ImageNet</a>: <em>ImageNet Large-scale Visual Recognition Challenge</em>,</li> <li>Miniplaces: Provided by Professor Bolei Zhou for his COM SCI 188 course (Winter 2023), a subset of <a href="http://places2.csail.mit.edu/">Places365</a>, and</li> <li><a href="https://www.kaggle.com/datasets/splcher/animefacedataset">Anime Faces</a>: A <em>Kaggle</em> dataset of anime faces.</li> </ul> <p>We normalize all data to \([-1, 1]\) and perform random horizontal flip as the only data augmentation.</p> <h3 id="network">Network</h3> <p>For our neural network \(\boldsymbol{\epsilon}_\theta\), we use the model design heuristic of DDPMs, that is, a denoising U-Net (i.e., matching input and output sizes), with additional self-attention blocks at the \(16 \times 16\) resolution between the convolutional blocks.</p> <p>A lot of code below takes inspiration from [1C], though we do make many structural changes that fit the repository better.</p> <h4 id="time-embeddings">Time Embeddings</h4> <p>We use time embeddings from the original ‚ÄúAttention is All You Need‚Äù paper.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">time_embed</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">):</span>
    <span class="n">half_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">exp_factor</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">half_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">embed</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">half_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">mul</span><span class="p">(</span><span class="o">-</span><span class="n">exp_factor</span><span class="p">))</span>
    <span class="n">embed</span> <span class="o">=</span> <span class="n">embed</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># Move embeddings to GPU (if possible)
</span>    <span class="n">embed</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">embed</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># Pair-wise multiplication
</span>    <span class="n">embed</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">embed</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">embed</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Zero padding if embed_dim is odd
</span>        <span class="n">embed</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">embed</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">embed</span>
</code></pre></div></div> <h4 id="residual-blocks">Residual Blocks</h4> <p>We employ residual blocks with two convolutions, group normalization, optional dropout, and optional \(3 \times 3\) convolutions for skip connections of mistmatching input and output channels (\(1 \times 1\) is the default for the ResNet). Each residual block also takes sinusoidal time embeddings and projects it through a linear layer to be summed with the feature map from the first convolution, as ‚Äòtime‚Äô (i.e., \(t\) in \(\mathbf{x}_t\)) is an extremely important indicator for noise. We use \(\textrm{Swish}\) as the activation function.</p> <p>During testing we found that replacing group normalization for batch normalization in small networks can work better for enhancing detail at the cost of global coherence, which may be beneficial for datasets like Flowers102. Thus, for the <code class="language-plaintext highlighter-rouge">group_norm</code> function we added the option to set <code class="language-plaintext highlighter-rouge">num_groups=-1</code> that defaults to batch normalization.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ResnetBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">do_conv_skip</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                 <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">time_embed_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="n">out_channels</span> <span class="o">=</span> <span class="n">in_channels</span> <span class="k">if</span> <span class="n">out_channels</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">out_channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>

        <span class="c1"># Convolution layer 1
</span>        <span class="n">self</span><span class="p">.</span><span class="n">norm_1</span> <span class="o">=</span> <span class="n">nutils</span><span class="p">.</span><span class="nf">group_norm</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">time_embed_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">time_embed_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
        <span class="c1"># Convolution layer 2
</span>        <span class="n">self</span><span class="p">.</span><span class="n">norm_2</span> <span class="o">=</span> <span class="n">nutils</span><span class="p">.</span><span class="nf">group_norm</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># ResNet skip layer (with 3x3 kernel option available with do_conv_skip)
</span>        <span class="k">if</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">3</span> <span class="k">if</span> <span class="n">do_conv_skip</span> <span class="k">else</span> <span class="mi">1</span>  <span class="c1"># Skip with larger kernel
</span>            <span class="n">self</span><span class="p">.</span><span class="n">conv_skip</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                                       <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">time_embed</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># Order for each layer: norm -&gt; activation -&gt; conv
</span>        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm_1</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">nutils</span><span class="p">.</span><span class="nf">swish</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv_1</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

        <span class="n">time_embed</span> <span class="o">=</span> <span class="n">nutils</span><span class="p">.</span><span class="nf">swish</span><span class="p">(</span><span class="n">time_embed</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">time_embed_proj</span><span class="p">(</span><span class="n">time_embed</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>  <span class="c1"># Apply to each channel
</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm_2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">nutils</span><span class="p">.</span><span class="nf">swish</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>  <span class="c1"># Apply dropout on second convolution layer
</span>        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv_2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">!=</span> <span class="n">self</span><span class="p">.</span><span class="n">out_channels</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv_skip</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">h</span>
        <span class="k">return</span> <span class="n">h</span>
</code></pre></div></div> <h4 id="attention-blocks">Attention Blocks</h4> <p>At the \(16 \times 16\) resolution level, we apply a convolution self-attention block with a skip connection. We also apply self-attention between the two residual blocks in the middle of the U-Net. Notice that, since we set <code class="language-plaintext highlighter-rouge">kernel_size=1</code> and <code class="language-plaintext highlighter-rouge">stride=1</code> for the \(Q\), \(K\), and \(V\) convolutions, we essentially have an element-wise linear layer with weights shared across each entry on the feature map (across channels).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AttentionBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nutils</span><span class="p">.</span><span class="nf">group_norm</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">Q</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">K</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">V</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

        <span class="c1"># Compute attention
</span>        <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span> <span class="o">*</span> <span class="n">W</span><span class="p">).</span><span class="nf">moveaxis</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># B C H W -&gt; B (H W) C
</span>        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>  <span class="c1"># B C H W -&gt; B C (H W)
</span>        <span class="n">w</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span>  <span class="c1"># Batched scores, B (H W) (H W), w[b, i, j] = sum_c q[b, i, c] k[b, c, j]
</span>        <span class="n">w</span><span class="p">.</span><span class="nf">mul_</span><span class="p">(</span><span class="nf">pow</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">))</span>  <span class="c1"># Normalize scores
</span>        <span class="n">w</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Apply softmax in preparation for summing
</span>
        <span class="c1"># Apply attention to values
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>  <span class="c1"># B C H W -&gt; B C (H W)
</span>        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="p">.</span><span class="nf">moveaxis</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># B (H W of q) (H W of k) -&gt; B (H W of k) (H W of q)
</span>        <span class="n">h</span> <span class="o">=</span> <span class="n">v</span> <span class="o">@</span> <span class="n">w</span>  <span class="c1"># Batched attention, B C (H W) (H W of q), sum_i v[b, c, i] w[b, i, j]
</span>        <span class="n">h</span> <span class="o">=</span> <span class="n">h</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>  <span class="c1"># ^ Taking linear combination of values weighted by cores
</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">out_proj</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">h</span>  <span class="c1"># Residual skip connection
</span>        <span class="k">return</span> <span class="n">h</span>
</code></pre></div></div> <h4 id="u-net">U-Net</h4> <p>The U-Net architecture is similar to that of the PixelCNN++, with the addition of the attention layers mentioned above. Particularly, we employ long skip connections between every pair of residual blocks (one in the downsampling layers and one in the mirroring upsampling layers). Each resolution level has <code class="language-plaintext highlighter-rouge">num_blocks</code> number of residual blocks before down/upsampling (usually <code class="language-plaintext highlighter-rouge">num_blocks=2</code>). Note that we do not include the code for the classes <code class="language-plaintext highlighter-rouge">Downsample</code> and <code class="language-plaintext highlighter-rouge">Upsample</code> here, but they are merely linearly-interpolated down/upsampling layers with optional \(3 \times 3\) convolutions afterwards.</p> <p>Additionally, we initialize the final output convolution layer with zeros, as inspired by [2C]. This improves the stability of the first few iterations as now it begins by predicting zeros, i.e., the mean of the noise prediction as \(\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})\), so the loss begins at (approximately) \(1\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">UNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_shape</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="n">num_blocks</span><span class="p">,</span> <span class="n">channel_mults</span><span class="p">,</span> <span class="n">attention_sizes</span><span class="p">,</span>
                 <span class="n">time_embed_channels</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">do_conv_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="k">assert</span> <span class="n">in_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Input shape must be square.</span><span class="sh">"</span>

        <span class="n">self</span><span class="p">.</span><span class="n">in_shape</span> <span class="o">=</span> <span class="n">in_shape</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_channels</span> <span class="o">=</span> <span class="n">hidden_channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_sizes</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">channel_mults</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_blocks</span> <span class="o">=</span> <span class="n">num_blocks</span>

        <span class="c1"># Time embedding
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">time_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Module</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">time_embed</span><span class="p">.</span><span class="n">fn</span> <span class="o">=</span> <span class="nf">partial</span><span class="p">(</span><span class="n">nutils</span><span class="p">.</span><span class="n">time_embed</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="n">hidden_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">time_embed</span><span class="p">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_channels</span><span class="p">,</span> <span class="n">time_embed_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">time_embed_channels</span><span class="p">,</span> <span class="n">time_embed_channels</span><span class="p">)])</span>

        <span class="c1"># Downsampling layers
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">current_size</span> <span class="o">=</span> <span class="n">in_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">in_channel_mults</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">channel_mults</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">()</span>
        <span class="n">in_channels_block</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">channel_mults</span><span class="p">)):</span>
            <span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">()</span>
            <span class="n">attentions</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">()</span>
            <span class="n">in_channels_block</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">hidden_channels</span> <span class="o">*</span> <span class="n">in_channel_mults</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">out_channels_block</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">hidden_channels</span> <span class="o">*</span> <span class="n">channel_mults</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="c1"># Add num_blocks Resnet blocks (with Attention blocks)
</span>            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>
                <span class="n">blocks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nc">ResnetBlock</span><span class="p">(</span><span class="n">in_channels_block</span><span class="p">,</span> <span class="n">out_channels_block</span><span class="p">,</span>
                                          <span class="n">time_embed_channels</span><span class="o">=</span><span class="n">time_embed_channels</span><span class="p">,</span>
                                          <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">))</span>
                <span class="n">in_channels_block</span> <span class="o">=</span> <span class="n">out_channels_block</span>
                <span class="k">if</span> <span class="n">current_size</span> <span class="ow">in</span> <span class="n">attention_sizes</span><span class="p">:</span>
                    <span class="n">attentions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nc">AttentionBlock</span><span class="p">(</span><span class="n">in_channels_block</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">))</span>
            <span class="c1"># Create down layer as nn.Module
</span>            <span class="n">down_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Module</span><span class="p">()</span>
            <span class="n">down_layer</span><span class="p">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">blocks</span>
            <span class="n">down_layer</span><span class="p">.</span><span class="n">attentions</span> <span class="o">=</span> <span class="n">attentions</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nf">len</span><span class="p">(</span><span class="n">channel_mults</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Downsample unless at last layer
</span>                <span class="n">down_layer</span><span class="p">.</span><span class="n">downsample</span> <span class="o">=</span> <span class="nc">Downsample</span><span class="p">(</span><span class="n">in_channels_block</span><span class="p">,</span> <span class="n">do_conv</span><span class="o">=</span><span class="n">do_conv_sample</span><span class="p">)</span>
                <span class="n">current_size</span> <span class="o">=</span> <span class="n">current_size</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="n">self</span><span class="p">.</span><span class="n">down_layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">down_layer</span><span class="p">)</span>

        <span class="c1"># Middle layers
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mid_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mid_layers</span><span class="p">.</span><span class="n">block_1</span> <span class="o">=</span> <span class="nc">ResnetBlock</span><span class="p">(</span><span class="n">in_channels_block</span><span class="p">,</span> <span class="n">in_channels_block</span><span class="p">,</span>
                                              <span class="n">time_embed_channels</span><span class="o">=</span><span class="n">time_embed_channels</span><span class="p">,</span>
                                              <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mid_layers</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="nc">AttentionBlock</span><span class="p">(</span><span class="n">in_channels_block</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mid_layers</span><span class="p">.</span><span class="n">block_2</span> <span class="o">=</span> <span class="nc">ResnetBlock</span><span class="p">(</span><span class="n">in_channels_block</span><span class="p">,</span> <span class="n">in_channels_block</span><span class="p">,</span>
                                              <span class="n">time_embed_channels</span><span class="o">=</span><span class="n">time_embed_channels</span><span class="p">,</span>
                                              <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">)</span>

        <span class="c1"># Upsampling layers
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">channel_mults</span><span class="p">))):</span>
            <span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">()</span>
            <span class="n">attentions</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">()</span>
            <span class="n">out_channels_block</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">hidden_channels</span> <span class="o">*</span> <span class="n">channel_mults</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">in_channels_skip</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">hidden_channels</span> <span class="o">*</span> <span class="n">channel_mults</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_blocks</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="n">num_blocks</span><span class="p">:</span>
                    <span class="n">in_channels_skip</span> <span class="o">=</span> <span class="n">hidden_channels</span> <span class="o">*</span> <span class="n">in_channel_mults</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">blocks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nc">ResnetBlock</span><span class="p">(</span><span class="n">in_channels_block</span> <span class="o">+</span> <span class="n">in_channels_skip</span><span class="p">,</span> <span class="n">out_channels_block</span><span class="p">,</span>
                                          <span class="n">time_embed_channels</span><span class="o">=</span><span class="n">time_embed_channels</span><span class="p">,</span>
                                          <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">))</span>
                <span class="n">in_channels_block</span> <span class="o">=</span> <span class="n">out_channels_block</span>
                <span class="k">if</span> <span class="n">current_size</span> <span class="ow">in</span> <span class="n">attention_sizes</span><span class="p">:</span>
                    <span class="n">attentions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nc">AttentionBlock</span><span class="p">(</span><span class="n">in_channels_block</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">))</span>
            <span class="c1"># Create up layer as nn.Module
</span>            <span class="n">up_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Module</span><span class="p">()</span>
            <span class="n">up_layer</span><span class="p">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">blocks</span>
            <span class="n">up_layer</span><span class="p">.</span><span class="n">attentions</span> <span class="o">=</span> <span class="n">attentions</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">up_layer</span><span class="p">.</span><span class="n">upsample</span> <span class="o">=</span> <span class="nc">Upsample</span><span class="p">(</span><span class="n">in_channels_block</span><span class="p">,</span> <span class="n">do_conv</span><span class="o">=</span><span class="n">do_conv_sample</span><span class="p">)</span>
                <span class="n">current_size</span> <span class="o">*=</span> <span class="mi">2</span>
            <span class="n">self</span><span class="p">.</span><span class="n">up_layers</span><span class="p">.</span><span class="nf">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">up_layer</span><span class="p">)</span>

        <span class="c1"># End layers
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_norm</span> <span class="o">=</span> <span class="n">nutils</span><span class="p">.</span><span class="nf">group_norm</span><span class="p">(</span><span class="n">in_channels_block</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels_block</span><span class="p">,</span> <span class="n">in_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                  <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_conv</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">fill_</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nf">list</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:])</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">in_shape</span><span class="p">,</span> \
               <span class="sa">f</span><span class="sh">"</span><span class="s">Shape of x </span><span class="si">{</span><span class="nf">tuple</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s"> does not match network definition.</span><span class="sh">"</span>

        <span class="c1"># Time embedding
</span>        <span class="n">t_embed</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">time_embed</span><span class="p">.</span><span class="nf">fn</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="n">t_embed</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">time_embed</span><span class="p">.</span><span class="n">dense</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">t_embed</span><span class="p">)</span>
        <span class="n">t_embed</span> <span class="o">=</span> <span class="n">nutils</span><span class="p">.</span><span class="nf">swish</span><span class="p">(</span><span class="n">t_embed</span><span class="p">)</span>
        <span class="n">t_embed</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">time_embed</span><span class="p">.</span><span class="n">dense</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">t_embed</span><span class="p">)</span>

        <span class="c1"># Downsampling
</span>
        <span class="n">h_skip</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="nf">in_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_sizes</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_blocks</span><span class="p">):</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">down_layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">blocks</span><span class="p">[</span><span class="n">j</span><span class="p">](</span><span class="n">h_skip</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">t_embed</span><span class="p">)</span>
                <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down_layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">attentions</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Apply attention heads
</span>                    <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">down_layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">attentions</span><span class="p">[</span><span class="n">j</span><span class="p">](</span><span class="n">h</span><span class="p">)</span>
                <span class="n">h_skip</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">self</span><span class="p">.</span><span class="n">num_sizes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">down_layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">downsample</span><span class="p">(</span><span class="n">h_skip</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">h_skip</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

        <span class="c1"># Middle
</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">h_skip</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">mid_layers</span><span class="p">.</span><span class="nf">block_1</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">t_embed</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">mid_layers</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">mid_layers</span><span class="p">.</span><span class="nf">block_2</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">t_embed</span><span class="p">)</span>

        <span class="c1"># Upsampling
</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_sizes</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_blocks</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">h</span><span class="p">,</span> <span class="n">h_skip</span><span class="p">.</span><span class="nf">pop</span><span class="p">()],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># Concatenate with skip at channel
</span>                <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">up_layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">blocks</span><span class="p">[</span><span class="n">j</span><span class="p">](</span><span class="n">h</span><span class="p">,</span> <span class="n">t_embed</span><span class="p">)</span>
                <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">attentions</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Apply attention heads
</span>                    <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">up_layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">attentions</span><span class="p">[</span><span class="n">j</span><span class="p">](</span><span class="n">h</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">up_layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">upsample</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

        <span class="c1"># End
</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">out_norm</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">nutils</span><span class="p">.</span><span class="nf">swish</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">out_conv</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">h</span>
</code></pre></div></div> <h3 id="diffusion">Diffusion</h3> <p>We follow the training and inference processes as defined by the original DDPM and DDIM paper. Particularly, we use the code structure from [3C] and the methods from [1C]. Many of the code snippets presented below are within the <code class="language-plaintext highlighter-rouge">Diffusion</code> runner class.</p> <h4 id="forward-process">Forward Process</h4> <p>The noise-adding function, or <code class="language-plaintext highlighter-rouge">q_sample</code> from the forward process \(q(\mathbf{x}_t \mid \mathbf{x}_{t - 1}, \, \mathbf{x}_0)\) as described in [1], adds the scheduled noise by first computing \(\alpha_t\) and then passing the noisy inputs through the network \(\boldsymbol{\epsilon}_\theta\) to obtain the predicted noise.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">q_sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">e</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    We express x_t as a linear combination of x_0 and noise e because
        q(x_t | x_0) = N(x_t; sqrt(a_t) x_0, (1 - a_t) I) .
    This is the key difference between DDPM and DDIM. The former tries to approximate e where
        x_(t - 1) + e = x_t ,
    whereas the latter mixes e and x_0 via a_t (see above). Because we are now using x_0, this
    is no longer a Markov chain, and during the p_sample process we can speed up the sampling
    process by skipping an arbitrary number of t each time just by parameterizing a_t.

    For more information: https://strikingloo.github.io/wiki/ddim
    </span><span class="sh">"""</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">betas</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">a_t</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">b</span><span class="p">).</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">index_select</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">t</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
        <span class="n">x_t</span> <span class="o">=</span> <span class="n">a_t</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">()</span> <span class="o">*</span> <span class="n">x_0</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">a_t</span><span class="p">).</span><span class="nf">sqrt</span><span class="p">()</span> <span class="o">*</span> <span class="n">e</span>  <span class="c1"># DDIM Eq. 4
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>  <span class="c1"># Predicted e
</span>    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <h4 id="training">Training</h4> <p>The training function calls <code class="language-plaintext highlighter-rouge">q_sample</code> and computes the error between the predicted noise and added noise. Notably, even though both the DDPM and DDIM paper defines the error as the \(L_2\) distance between the output \(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\) and added noise \(\boldsymbol{\epsilon}\), in practice we found, for much fewer training iterations, using \(L_1\) distance as the loss function both improves detail and reduces artifacts. Thus, we use \(L_1\) loss, i.e., mean absolute error, instead.</p> <p>Interestingly, with smaller models, \(L_2\) loss actually causes bad colorcasting, especially during earlier iterations, whereas \(L_1\) loss does not.</p> <p>The below is an abridged version of the code that removes all the auxiliary elements not directly relevant to the training logic, e.g., dataset/loader initialization, optimizer, exponential moving average (EMA), <code class="language-plaintext highlighter-rouge">tqdm</code>, TensorBoard, model checkpoint saving, etc. To see details of auxiliary code, check the linked official code repository of this project.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">config</span>

    <span class="n">train_loader</span> <span class="o">=</span>  <span class="c1"># Initialize PyTorch training loader with data
</span>
    <span class="n">network</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">network</span>

    <span class="n">optimizer</span> <span class="o">=</span>  <span class="c1"># Initialie optimizer (usually Adam)
</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">config</span><span class="p">.</span><span class="n">training</span><span class="p">.</span><span class="n">num_i</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

            <span class="n">n</span> <span class="o">=</span> <span class="n">x_0</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">network</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

            <span class="n">x_0</span> <span class="o">=</span> <span class="n">x_0</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">x_0</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">data_transform</span><span class="p">(</span><span class="n">x_0</span><span class="p">)</span>
            <span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_0</span><span class="p">)</span>  <span class="c1"># Noise to mix with x_0 to create x_t
</span>
            <span class="c1"># Arithmetic sampling
</span>            <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">diffusion</span><span class="p">.</span><span class="n">num_t</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,),</span>
                              <span class="n">device</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">t</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">diffusion</span><span class="p">.</span><span class="n">num_t</span> <span class="o">-</span> <span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)[:</span><span class="n">n</span><span class="p">]</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_sample</span><span class="p">(</span><span class="n">x_0</span><span class="o">=</span><span class="n">x_0</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">e</span><span class="o">=</span><span class="n">e</span><span class="p">)</span>  <span class="c1"># Estimate noise added
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">rutils</span><span class="p">.</span><span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">training</span><span class="p">.</span><span class="n">criterion</span><span class="p">)</span>

            <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

            <span class="k">try</span><span class="p">:</span>  <span class="c1"># Perform gradient clipping only if defined in config
</span>                <span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">clip_grad_norm_</span><span class="p">(</span><span class="n">network</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">config</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="n">gradient_clip</span><span class="p">)</span>
            <span class="k">except</span> <span class="nb">AttributeError</span><span class="p">:</span>
                <span class="k">pass</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">config</span><span class="p">.</span><span class="n">training</span><span class="p">.</span><span class="n">num_i</span><span class="p">:</span>  <span class="c1"># Training for exactly num_i iterations
</span>                <span class="k">break</span>
</code></pre></div></div> <h4 id="sampling">Sampling</h4> <p>The sampling function is the same as described above in the introduction. Note that we add the option to return the entire generation sequence, including the progressively denoised \(\mathbf{x}_t\) and \(\mathbf{x}_0\) predictions computed from the network‚Äôs noise prediction. We also removed some auxiliary code elements here to focus on the sampling algorithm itself.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">p_sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">num_t</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">num_t_steps</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">skip_type</span><span class="o">=</span><span class="sh">"</span><span class="s">uniform</span><span class="sh">"</span><span class="p">,</span>
             <span class="n">eta</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">ema</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sequence</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">config</span>

    <span class="n">network</span> <span class="o">=</span> <span class="n">utils</span><span class="p">.</span><span class="nf">get_default</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">)</span>
    <span class="n">network</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">skip_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">uniform</span><span class="sh">"</span><span class="p">:</span>
        <span class="n">t_skip</span> <span class="o">=</span> <span class="n">num_t</span> <span class="o">//</span> <span class="n">num_t_steps</span>
        <span class="n">t_sequence</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_t</span><span class="p">,</span> <span class="n">t_skip</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">skip_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">quadratic</span><span class="sh">"</span><span class="p">:</span>
        <span class="n">t_sequence</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">num_t</span><span class="p">),</span> <span class="n">num_t_steps</span><span class="p">))</span>
        <span class="n">t_sequence</span> <span class="o">=</span> <span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_sequence</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Time skip type </span><span class="si">{</span><span class="n">skip_type</span><span class="si">}</span><span class="s"> not supported.</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">betas</span>
    <span class="n">t_sequence_next</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">t_sequence</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x_0_predictions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">x_t_predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="nf">reversed</span><span class="p">(</span><span class="n">t_sequence</span><span class="p">),</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">t_sequence_next</span><span class="p">)):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">i</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># Same time across batch
</span>        <span class="n">t_next</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">j</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">a_t</span> <span class="o">=</span> <span class="n">rutils</span><span class="p">.</span><span class="nf">alpha</span><span class="p">(</span><span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">)</span>
        <span class="n">a_t_next</span> <span class="o">=</span> <span class="n">rutils</span><span class="p">.</span><span class="nf">alpha</span><span class="p">(</span><span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t_next</span><span class="p">)</span>

        <span class="n">x_t</span> <span class="o">=</span> <span class="n">x_t_predictions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">e_t</span> <span class="o">=</span> <span class="nf">network</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">)</span>

        <span class="n">x_0_t</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_t</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">a_t</span><span class="p">).</span><span class="nf">sqrt</span><span class="p">()</span> <span class="o">*</span> <span class="n">e_t</span><span class="p">)</span> <span class="o">/</span> <span class="n">a_t</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">()</span>  <span class="c1"># DDIM Eq. 12, "predicted x_0"
</span>        <span class="n">x_0_predictions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x_0_t</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">())</span>

        <span class="c1"># DDIM Eq. 16, s_t is constant for amount of random noise during generation.
</span>        <span class="c1"># If eta == 0, then we have DDIM; if eta == 1, then we have DDPM
</span>        <span class="n">s_t</span> <span class="o">=</span> <span class="n">eta</span> <span class="o">*</span> <span class="p">(((</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">a_t_next</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">a_t</span><span class="p">))</span> <span class="o">*</span> <span class="p">((</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">a_t</span><span class="p">)</span> <span class="o">/</span> <span class="n">a_t_next</span><span class="p">)).</span><span class="nf">sqrt</span><span class="p">()</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">s_t</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># DDIM Eq. 12, "random noise"
</span>
        <span class="c1"># DDIM Eq. 12, "direction pointing to x_t"
</span>        <span class="n">x_d</span> <span class="o">=</span> <span class="p">((</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">a_t_next</span><span class="p">)</span> <span class="o">-</span> <span class="n">s_t</span><span class="p">.</span><span class="nf">square</span><span class="p">()).</span><span class="nf">sqrt</span><span class="p">()</span> <span class="o">*</span> <span class="n">e_t</span>

        <span class="n">x_t_next</span> <span class="o">=</span> <span class="n">a_t_next</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">()</span> <span class="o">*</span> <span class="n">x_0_t</span> <span class="o">+</span> <span class="n">x_d</span> <span class="o">+</span> <span class="n">e</span>  <span class="c1"># DDIM Eq. 12
</span>        <span class="n">x_t_predictions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x_t_next</span><span class="p">)</span>  <span class="c1"># Only keep gradients of final x_t prediction
</span>        <span class="n">x_t_predictions</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_t_predictions</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">].</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">sequence</span><span class="p">:</span>  <span class="c1"># Only return final generated images
</span>        <span class="k">return</span> <span class="n">x_t_predictions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">x_t_predictions</span><span class="p">,</span> <span class="n">x_0_predictions</span>  <span class="c1"># Return entire generation process
</span></code></pre></div></div> <h2 id="inversion">Inversion</h2> <h3 id="optimization-based-method">Optimization-based Method</h3> <p>Heuristically, optimization-based methods are the simplest. Particularly, we implement the gradient-based method. Given some image \(\mathbf{x}\), we are minimizing</p> \[\mathcal{L}(\mathbf{z}) = \mathcal{R}(G(\mathbf{z}), \mathbf{x}) ,\] <p>where \(G\) is the short hand (‚Äúgenerator‚Äù) for the full DDIM inference process and \(\mathcal{R}\) is the reconstruction loss and can include</p> <ul> <li>\(L_1\) loss, where \(\mathcal{R}(\hat{\mathbf{x}}, \mathbf{x}) = \frac{1}{n} \sum_{i} \lvert \mathbf{x}_i - \hat{\mathbf{x}}_i \rvert\),</li> <li>\(L_2\) loss, where \(\mathcal{R}(\hat{\mathbf{x}}, \mathbf{x}) = \frac{1}{n} \sum_{i} (\mathbf{x}_i - \hat{\mathbf{x}}_i)^2\),</li> <li>\(L_\infty\) loss, where \(\mathcal{R}(\hat{\mathbf{x}}, \mathbf{x}) = \lvert \mathbf{x}_j - \hat{\mathbf{x}}_j \rvert\) where \(j = \max_i \lvert \mathbf{x}_i - \hat{\mathbf{x}}_i \rvert\),</li> <li>peak signal-to-noise ratio (PSNR), where \(\mathcal{R}(\hat{\mathbf{x}}, \mathbf{x}) = 10 \log_{10} \left( \frac{\textrm{MAX}^2}{\textrm{MSE}} \right)\), where \(\textrm{MAX}\) is the maximum possible value of the image (i.e., \(1\)) and \(\textrm{MSE}\) is the \(L_2\) loss, and</li> <li>structural similarity (SSIM), which is more complex to typeset and does not perform that well, so I will merely leave a <a href="https://en.wikipedia.org/wiki/Structural_similarity">link</a> to more information about it instead [2].</li> </ul> <p>Note that, for PSNR and SSIM, unlike \(L_p\) losses, the higher the value, the better the reconstruction, so we instead maximize \(\mathcal{L}\) in those cases.</p> <p>With the loss function defined, we can simply perform optimization (e.g., SGD, Adam, etc.) with the gradient \(\frac{\partial \mathcal{L}}{\mathbf{z}}\), iteratively updating \(\mathbf{z}\) until convergence, at which point \(\hat{\mathbf{x}} = G(\mathbf{z})\) should be a good reconstruction of \(\mathbf{x}\).</p> <p>Note that gradient-based methods are really only computationally viable with DDIMs, as we can set \(\Delta t\) to be large so the gradients do not have to propagate through much fewer passes of the network. Even with this optimization-based methods can be quite slow compared to learning-based methods.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradient_inversion</span><span class="p">(</span><span class="n">z</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">diffusion</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">num_i</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">num_t_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                       <span class="n">criterion</span><span class="o">=</span><span class="sh">"</span><span class="s">l1</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">diffusion</span><span class="p">.</span><span class="n">device</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">utils</span><span class="p">.</span><span class="nf">get_default</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="o">*</span><span class="n">target</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> <span class="n">diffusion</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">z</span>
    <span class="n">z</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">()</span>

    <span class="n">maximize</span> <span class="o">=</span> <span class="n">criterion</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">psnr</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">ssim</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span><span class="n">z</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">maximize</span><span class="o">=</span><span class="n">maximize</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_i</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">diffusion</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">z</span><span class="p">,</span> <span class="n">sequence</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">ema</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_t_steps</span><span class="o">=</span><span class="n">num_t_steps</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">rutils</span><span class="p">.</span><span class="nf">criterion</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">criterion</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">z</span>
</code></pre></div></div> <h3 id="learning-based-method">Learning-based Method</h3> <p>We experimented with learning-based methods with two types of objectives. Suppose we have an encoder \(E : \mathcal{X} \rightarrow \mathcal{Z}\) that maps some input image \(\mathbf{x} \in \mathcal{X}\) to its corresponding noise latent \(\mathbf{z} \in \mathcal{Z}\) such that \(G(\mathbf{z}) \approxi \mathbf{x}\). Then, we can either</p> <ol> <li>minimize \(\mathcal{R}(E(\mathbf{x}), \mathbf{z})\), i.e., minimizing the error within the latent space, and</li> <li>minimize \(\mathcal{R}(G(E(\mathbf{x})), \mathbf{x})\), i.e., minimizing the error within the image space (reconstruction loss),</li> </ol> <p>where \(\mathcal{R}\) can be any loss between two vectors such as \(L_2\) as mentioned in the previous subsubsection. We do not show results for both these methods as they do not work well <em>at all</em>, at least with the rough implementation we have in our code base, which is linked at the very top of this article. We discuss why this is the case in the Results section below.</p> <h3 id="hybrid-method">Hybrid Method</h3> <p>Because, as mentioned, the learning-based method does not work well, at least with our rudimentary implementation, we do not explore the hybrid method. Particularly, since the hybrid method uses some learning-based method as a noise initializer and then some optimization-based method to finetune the noise, if the noise initializer is not great to begin with, there is not much point to the hybrid method.</p> <h2 id="semantic-feature-editing">Semantic Feature Editing</h2> <p>Suppose we get good results from optimization-based inversion and interpolation (explored later). Then, we know this indicates that the DDIM, and more generally, diffusion models with deterministic inference, does have a one-to-one mapping between the noise latent space \(\mathcal{Z}\) and image space \(\mathcal{X}\) that is continuously ‚Äòmeaningful.‚Äô Since learning-based (and consequently hybrid-based) methods did not work, we explore another technique for manipulating the latent space of generative models, particularly GANs. Specifically, we replicate parts of [4] with the DDIM.</p> <p>Suppose we generate many noise-image pairs with our trained DDIM, so \(\mathbf{z}\) and its corresponding \(\mathbf{x} = G(\mathbf{z})\). Also, suppose that our original dataset has additional labels for each image in the dataset which correspond to binary features such as age, eyeglasses, masculinity. Then, we can do the following.</p> <ol> <li>First, train a multinomial classifier that can label these images with their corresponding binary attributes.</li> <li>Then, with each \(\mathbf{x}\), find its corresponding binary attributes with the trained classifier. Suppose the resulting attributes are \(f_1, \dots, f_m\) for \(m\) multinomial classes.</li> <li>Then, for any specific binary feature \(f_i\), we train a binary classifier \(\mathbf{w}^T \mathbf{z} + b = \hat{f}_i\) that finds a hyperplane in the noise latent space (which we can do as the \(\mathbf{z}\) and \(\mathbf{x}\) are paired) which separates the positive and negative instances of the chosen binary attribute.</li> </ol> <p>Notice that said hyperplane, when translated to contain the origin, is exactly described by \(\mathbf{w}\), as we compute points on a hyperplane with \(\mathbf{n} \cdot \mathbf{x} = \mathbf{n}^T \mathbf{x}\), where \(\mathbf{n}\) is the normal vector of the hyperplane. Importantly, \(\mathbf{w}\)‚Äîthe weights of the binary classifier‚Äîis exactly the normal vector of the hyperplane. In other words, \(\mathbf{w}\) describes the direction to nudge/push the noise latent \(\mathbf{z}\) that maximizes the binary feature \(f_i\), and correspondingly \(-\mathbf{w}\) describes the direction that minimizes said binary feature. To ensure that the ‚Äòamount‚Äô in which we nudge by \(\mathbf{w}\) is consistent across different binary classifier instances, we normalize the direction vector with \(\mathbf{u} = \frac{1}{\| \mathbf{w} \|} \mathbf{w}\). Therefore, by modifying \(\mathbf{z}\) with</p> \[\mathbf{z}' = \mathbf{z} + \alpha \mathbf{u}\] <p>for some \(\alpha \in \mathbb{R}\), we can semantically edit attributes/features of the paired image \(\mathbf{x}\) when we compute \(\mathbf{x}' = G(\mathbf{z}')\).</p> <h2 id="results">Results</h2> <h3 id="ddim-generation">DDIM Generation</h3> <div> <div class="column-half"> <img src="/assets/img/ddim_inversion/celeba_sample.png" width="100%"/> </div> <div class="column-half"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/celeba_sample.webm"/> </video> </div> </div> <div> <div class="column-half"> <img src="/assets/img/ddim_inversion/ffhq_sample.png" width="100%"/> </div> <div class="column-half"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/ffhq_sample.webm"/> </video> </div> </div> <div> <div class="column-half"> <img src="/assets/img/ddim_inversion/church_sample.png" width="100%"/> </div> <div class="column-half"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/church_sample.webm"/> </video> </div> </div> <div> <div class="column-half"> <img src="/assets/img/ddim_inversion/flowers102_sample.png" width="100%"/> </div> <div class="column-half"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/flowers102_sample.webm"/> </video> </div> </div> <div> <div class="column-half"> <img src="/assets/img/ddim_inversion/imagenet64_sample.png" width="100%"/> </div> <div class="column-half"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/imagenet64_sample.webm"/> </video> </div> </div> <div> <div class="column-half"> <img src="/assets/img/ddim_inversion/miniplaces_sample.png" width="100%"/> </div> <div class="column-half"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/miniplaces_sample.webm"/> </video> </div> </div> <div> <div class="column-half"> <img src="/assets/img/ddim_inversion/anime_sample.png" width="100%"/> </div> <div class="column-half"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/anime_sample.webm"/> </video> </div> </div> <p><em>Figure 2: Sample generated images with DDIM, with the final generated results (left) and evolution of some fixed latent over the training process with EMA parameters (right). We generate the samples with <code class="language-plaintext highlighter-rouge">num_t_step=50</code>. The datasets used, from the top to bottom, are CelebA, FFHQ, LSUN Churches, Flowers102, ImageNet, Miniplaces, and Anime Faces.</em></p> <p>We train DDIMs on the datasets CelebA, FFHQ, LSUN Churches, Flowers102, ImageNet, Miniplaces, and Anime Faces with identical architectures as the output images are all \(64 \times 64\). Particularly, we set filters of \([128, 256, 256, 256, 512]\) with self-attention at the \(16 \times 16\) resolution (so right after the second \(256\) layer), with each filter layer consisting of two residual blocks and then a down/upsampling module. We select \(128\) as the time embedding dimensions. We set a dropout of \(0.1\). We use \(3 \times 3\) convolutions for mismatching channel residual skips, and we also use \(3 \times 3\) convolutions (after interpolation) for down/upsampling. The model is about \(75.3\) million parameters. We did find significant generation quality improvements of the \(75\)-million model compared to much smaller \(5\)- to \(10\)-million parameter models, especially for LSUN Churches and Miniplaces.</p> <p>Following DDPM and DDIM, we set \(T = 1000\), \(\beta_1 = 0.0001\), and \(\beta_T = 0.02\) (\(\alpha_t = 1 - \beta_t\), here \(\alpha\) is notated as it is in DDPM, for DDIM we need to take the cumulative product as well), with a default sampling step of \(50\), so \(\Delta t = \frac{1000}{50 - 1}\). \(\sigma_t = 0\) for all \(t \in \{ 1, \dots, T \}\) for deterministic sampling.</p> <p>We use the Adam optimizer with learning rate \(0.0002\) and default hyperparameters for \(72000\) iterations with a batch size of \(64\). Doing so takes approximately \(11\) hours on a single 450W NVIDIA RTX 3090Ti with 24GB VRAM. We also clip the gradient entries to \(1.0\) to improve model stability. We also set an EMA of \(0.9995\) for the model parameters to further improve stability, which is less than DDPM and DDIM‚Äôs \(0.9999\) as they train for much more iterations. We do not use weight decay, as we found doing so decreases the output generation quality.</p> <h3 id="inversion-1">Inversion</h3> <h4 id="optimization-based-method-1">Optimization-based Method</h4> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/celeba_inversion_1_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/celeba_inversion_1_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/celeba_inversion_1_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/celeba_inversion_1_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/celeba_inversion_2_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/celeba_inversion_2_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/celeba_inversion_2_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/celeba_inversion_2_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/ffhq_inversion_1_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/ffhq_inversion_1_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/ffhq_inversion_1_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/ffhq_inversion_1_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/ffhq_inversion_2_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/ffhq_inversion_2_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/ffhq_inversion_2_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/ffhq_inversion_2_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/church_inversion_1_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/church_inversion_1_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/church_inversion_1_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/church_inversion_1_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/church_inversion_2_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/church_inversion_2_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/church_inversion_2_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/church_inversion_2_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/flowers102_inversion_1_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/flowers102_inversion_1_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/flowers102_inversion_1_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/flowers102_inversion_1_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/flowers102_inversion_2_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/flowers102_inversion_2_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/flowers102_inversion_2_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/flowers102_inversion_2_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/imagenet64_inversion_1_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/imagenet64_inversion_1_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/imagenet64_inversion_1_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/imagenet64_inversion_1_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/imagenet64_inversion_2_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/imagenet64_inversion_2_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/imagenet64_inversion_2_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/imagenet64_inversion_2_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/miniplaces_inversion_1_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/miniplaces_inversion_1_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/miniplaces_inversion_1_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/miniplaces_inversion_1_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/miniplaces_inversion_2_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/miniplaces_inversion_2_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/miniplaces_inversion_2_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/miniplaces_inversion_2_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/anime_inversion_1_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/anime_inversion_1_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/anime_inversion_1_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/anime_inversion_1_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/anime_inversion_2_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/anime_inversion_2_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/anime_inversion_2_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/anime_inversion_2_2.webm"/> </video> </div> </div> <p><em>Figure 3: Inversion with optimization-based method (using Adam optimizer), with the original image \(\mathbf{x}\) (left), reconstructed image \(\hat{\mathbf{x}}\) (middle), and evolution of the reconstruction during training (right). The reconstructed PSNR is generally around \(23 \textrm{dB}\) to \(29 \textrm{dB}\)</em>.</p> <p>We use the Adam optimizer with learning rate \(0.01\) and maximize PSNR loss. We set \(\Delta t = \frac{1000}{10 - 1}\) (i.e., <code class="language-plaintext highlighter-rouge">num_t_steps=10</code>) and optimize for \(300\) iterations. The initial latent vector \(\mathbf{z}\) is randomly initialized.</p> <p>Notice that the optimization-based method generally reconstructs the global features of the image but can be lacking in fine detail, which can be partly attributed to the small <code class="language-plaintext highlighter-rouge">num_t_steps</code> (i.e., large \(\Delta t\)) since increasing it generally leads to longer inversion time (as we need to backpropagate through more passes of the network), larger number of iterations (gradient vanishing starts to occur with more backpropagations), and more complex loss landscapes (i.e., a more difficult optimizationt task).</p> <p>Note that the images we test inversion on are all in the validation split of their respective datasets, that is, the images are not used to train the model.</p> <h4 id="learning-based-method-1">Learning-based Method</h4> <div> <div class="column-half"> <img src="/assets/img/ddim_inversion/celeba_inversion_encoder_1.png" width="100%"/> </div> <div class="column-half"> <img src="/assets/img/ddim_inversion/celeba_inversion_encoder_2.png" width="100%"/> </div> </div> <p><em>Figure 4: Inversion reconstruction examples with learning-based methods of option 1 (noise latent error, left) and option 2 (reconstruction error, right) after training the encoder for \(\sim 10000\) iterations.</em></p> <p>As mentioned in the Methods section, the learning-based method does not work well, as least with our rough experiments. Implementation-wise, we slightly modified the base U-Net so that it can be used without time embeddings. Then, we simply initialize a smaller version of our U-Net as the encoder.) Particularly, the PSNR of the reconstruction never exceeds \(\sim 8\) and the loss between \(E(\mathbf{x})\) and \(\mathbf{x}\) very quickly plateaus. Visually, we see blurry outlines that vaguely resemble faces and/or heads with strong color shifts, even after many iterations. We analyze why this may be the case below.</p> <ol> <li>Minimizing noise latent error: From [2], we see that no studies have tried solely optimizing for error within the latent space. This is likely because that, not only is the latent space of diffusion models very high-dimensional, it is also, just like the \(\mathcal{Z}\) space of GANs, very irregular and sensitive to small changes. Hence, at least with the current noise-image pair brute-force training method without much hyperparameter searching, it is not surprisingly that this method does not work well.</li> <li>Minimizing reconstruction error: Since the loss function directly involves \(G\), we must backpropagate through the generation function. Here, unlike optimization-based methods where we backpropagate through the model with a batch size of \(1\) as we only want to optimize the latent of one image, since we are training an encoder \(E\) with learning-based methods, we must instead backpropagate through the model <code class="language-plaintext highlighter-rouge">num_t_steps</code> times for some nontrivial batch size (e.g., \(64\)). This is a huge memory requirement, one that is way more than that required for training the DDIM, especially if <code class="language-plaintext highlighter-rouge">num_t_steps</code> is large, as even training the DDIM only requires backpropagation through the model once. Hence, we tried training an encoder with a small batch size \(\sim 4\) (as that was the largest batch size trainable on our GPU), and, as expected, the results were simply terrible. Gradient vanishing also becomes problematic with larger <code class="language-plaintext highlighter-rouge">num_t_steps</code> as well, which may explain the stagnating learning.</li> </ol> <p>There are still many learning-based methods that train an encoder for GANs, but we must remember that the \(\mathcal{Z}\)-space for GANs is very low-dimensional, which is not the case for diffusion models. Though our experiments and results are only preliminary, it echoes how there are few learning-based methods covered in [2]. Perhaps combining both options can produce better results, but, judging from the current reconstruction examples, we likely need to be a lot more careful designing learning-based methods for DDIMs.</p> <h4 id="hybrid-method-1">Hybrid Method</h4> <p>As mentioned in the Methods section, because learning-based methods do not work well from our experiments, we do not explore the hybrid method in this article. Particularly, getting to a PSNR of \(\sim 8\) usually only takes about one to two optimization-based inversion iterations, which renders the current state of the learning-based method irrelevant for this article.</p> <h3 id="interpolation">Interpolation</h3> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/celeba_interpolation_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/celeba_interpolation_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/celeba_interpolation_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/celeba_interpolation_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/ffhq_interpolation_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/ffhq_interpolation_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/ffhq_interpolation_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/ffhq_interpolation_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/church_interpolation_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/church_interpolation_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/church_interpolation_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/church_interpolation_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/flowers102_interpolation_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/flowers102_interpolation_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/flowers102_interpolation_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/flowers102_interpolation_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/imagenet64_interpolation_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/imagenet64_interpolation_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/imagenet64_interpolation_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/imagenet64_interpolation_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/miniplaces_interpolation_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/miniplaces_interpolation_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/miniplaces_interpolation_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/miniplaces_interpolation_2.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/anime_interpolation_1.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/anime_interpolation_1.webm"/> </video> </div> </div> <div> <div class="column-twothird"> <img src="/assets/img/ddim_inversion/anime_interpolation_2.png" width="100%"/> </div> <div class="column-third"> <video class="square" width="100%" muted="" controls=""> <source src="/assets/img/ddim_inversion/anime_interpolation_2.webm"/> </video> </div> </div> <p><em>Figure 5: Interpolating between two inverted latent vectors and the generated results, with image at \(\alpha = 0\), image at \(\alpha = 1\), and the interpolation animation of intermediate \(\alpha\) values.</em></p> <p>Suppose we have two images \(\mathbf{x}_1\) and \(\mathbf{x}_2\) and their respective inverted latent vector \(\mathbf{z}_1\) and \(\mathbf{z}_2\), we can interpolate \(\mathbf{x}_1\) and \(\mathbf{x}_2\) in image space by interpolating \(\mathbf{z}_1\) and \(\mathbf{z}_2\) in latent space. Particularly, we assume \(\mathbf{z} \sim \mathcal{N}(0, \mathbf{I})\), linear interpolation does not maintain the magnitude of \(\mathbf{z}\) (e.g., consider \(\mathbf{z}_1 = -\mathbf{z}_2\) as an extreme example), so we instead use spherical interpolation inspired by [1]. Given two latent vectors \(\mathbf{z}_1\) and \(\mathbf{z}_2\) and some \(\alpha \in [0, 1]\), where \(\alpha = 0\) produces \(\mathbf{z}_1\) and \(\alpha = 1\) produces \(\mathbf{z}_2\), we interpolate with</p> \[\theta = \arccos\left( \frac{\mathbf{z}_1 \cdot \mathbf{z}_2}{\| \mathbf{z}_1 \|_2 \| \mathbf{z}_2 \|_2} \right) \\ \hat{\mathbf{z}} = \frac{\sin((1 - \alpha) \theta)}{\sin(\theta)} \mathbf{z}_1 + \frac{\sin(\alpha \theta)}{\sin(\theta)} \mathbf{z}_2 ,\] <p>where \(\hat{\mathbf{z}}\) is the result of the interpolation that we can then use to generate the interpolated images.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">slerp</span><span class="p">(</span><span class="n">z_1</span><span class="p">,</span> <span class="n">z_2</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">acos</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">z_1</span> <span class="o">*</span> <span class="n">z_2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">z_1</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">z_2</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">((</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">*</span> <span class="n">z_1</span> <span class="o">+</span> \
        <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">*</span> <span class="n">z_2</span>


<span class="k">def</span> <span class="nf">interpolation</span><span class="p">(</span><span class="n">z_1</span><span class="p">,</span> <span class="n">z_2</span><span class="p">,</span> <span class="n">diffusion</span><span class="p">,</span> <span class="n">num_t_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_alphas</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">x_mixes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">num_alphas</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
        <span class="n">z_mix</span> <span class="o">=</span> <span class="nf">slerp</span><span class="p">(</span><span class="n">z_1</span><span class="p">,</span> <span class="n">z_2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
        <span class="n">x_mix</span> <span class="o">=</span> <span class="n">diffusion</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">z_mix</span><span class="p">,</span> <span class="n">sequence</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">num_t_steps</span><span class="o">=</span><span class="n">num_t_steps</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x_mixes</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x_mix</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">x_mixes</span>
</code></pre></div></div> <h3 id="semantic-feature-editing-1">Semantic Feature Editing</h3> <p style="text-align: center;"> Attribute: Age <img src="/assets/img/ddim_inversion/celeba_editing_age_1.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_age_2.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_age_3.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_age_4.png" width="100%"/> Attribute: Attractiveness <img src="/assets/img/ddim_inversion/celeba_editing_attractive_1.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_attractive_2.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_attractive_3.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_attractive_4.png" width="100%"/> Attribute: Blonde hair <img src="/assets/img/ddim_inversion/celeba_editing_blond_1.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_blond_2.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_blond_3.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_blond_4.png" width="100%"/> Attribute: Eyeglasses <img src="/assets/img/ddim_inversion/celeba_editing_eyeglasses_1.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_eyeglasses_2.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_eyeglasses_3.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_eyeglasses_4.png" width="100%"/> Attribute: Masculinity <img src="/assets/img/ddim_inversion/celeba_editing_masculinity_1.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_masculinity_2.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_masculinity_3.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_masculinity_4.png" width="100%"/> Attribute: Smile <img src="/assets/img/ddim_inversion/celeba_editing_smile_1.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_smile_2.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_smile_3.png" width="100%"/> <img src="/assets/img/ddim_inversion/celeba_editing_smile_4.png" width="100%"/> </p> <p><em>Figure 6: Semantic editing of generated faces for CelebA. Note that the ranges for \(\alpha\) is different for every sample as we manually try different values for each and pick the one which showcases the range of the editing the best.</em></p> <p>We apply semantic feature editing on the CelebA dataset, as this is the only dataset we train DDIMs on which has extensive binary attribute annotations. There are \(40\) binary attributes, with notable ones including age, attractiveness (yikes), eyeglasses, masculinity, and smile. From the dataset, we train a multinomial classifier over the entire training dataset for all \(40\) binary attributes. For the multinomial classifier, we adopt the ResNet-20 architecture from [5] originally for CIFAR-10 classification, but we add an additional \(5 \times 5\) convolution with stride \(2\) to downsample the input images from \(64 \times 64\) to \(32 \times 32\), matching CIFAR-10. We use the same optimizer (SGD with Nesterov momentum) and learning rate scheduling (\(0.1\), drop by scale \(0.1\) at half-way and three-fourth point) as [5] and train for \(60000\) iterations.</p> <p>Then, we pre-generate \(262144\) noise-image pairs. Note that, for this section, we set <code class="language-plaintext highlighter-rouge">num_t_steps=50</code> for better generation detail (and that we are no longer backpropagating through the DDIM). We train a logistic regression classifier on each of the selected binary attributes we show above with these noise-image pairs, obtaining the direction vector (which is also the weight vector) \(\mathbf{w}\) that we normalize to obtain \(\mathbf{u}\). We train with SGD with Nesterov momentum and similar learning rate scheduling as above, with a weight decay of \(0.001\), and we train for \(10000\) iterations. (Note that, unlike [4], we do not pick the top and bottom \(10\)K images by binary attribute score and train the binary classifier on that, but we simply train on all \(262\)K images for simplicity. We also use logistic regression instead of a support vector machine, unlike [4], as our experiments with SVMs all did not work, likely due to not picking out the top and bottom \(10\)K images.) Then, we perform semantic image editing by pushing some \(\mathbf{z}\) with \(\mathbf{u}\) by \(\alpha\), as discussed in the Methods section.</p> <p>Notice that, with many of these editing examples, the interpolated direction/attribute is not necessarily disentangled from other image features, which echoes [4] with semantic editing within the \(\mathcal{Z}\) space.</p> <h4 id="visualizing-the-decision-boundary">Visualizing the Decision Boundary</h4> <table class="table-none" style="table-layout: fixed; width: 100%"> <tr style="text-align: center;"> <td>Age</td> <td>Attractiveness</td> <td>Blonde hair</td> <td>Eyeglasses</td> <td>Masculinity</td> <td>Smile</td> </tr> <tr> <td><img src="/assets/img/ddim_inversion/celeba_editing_age_u.png" width="100%"/></td> <td><img src="/assets/img/ddim_inversion/celeba_editing_attractive_u.png" width="100%"/></td> <td><img src="/assets/img/ddim_inversion/celeba_editing_blond_u.png" width="100%"/></td> <td><img src="/assets/img/ddim_inversion/celeba_editing_eyeglasses_u.png" width="100%"/></td> <td><img src="/assets/img/ddim_inversion/celeba_editing_masculinity_u.png" width="100%"/></td> <td><img src="/assets/img/ddim_inversion/celeba_editing_smile_u.png" width="100%"/></td> </tr> </table> <p><em>Figure 7: Decision boundaries/direction vector \(\mathbf{u}\) of the binary attributes above, normalized to \([0, 1]\) for display.</em></p> <p>Since the decision boundary or direction vector, \(\mathbf{u}\), is the same dimensions as the image by nature of the diffusion model, we can visualize it to see how it is pushing the noise latents to achieve semantic feature editing. Fascinatingly, we see that \(\mathbf{u}\) does indeed encode visual information, which is sensible, as the denoising inference process of diffusion models mean that we can add visual features we want directly to the noise latents to ‚Äòguide‚Äô the DDIM to generate the desired outputs. Although this <em>can</em> be done manually, the semantic feature editing method provides us a way to do so with a data-driven approach.</p> <p>We see that the decision boundaries does indeed semantically correspond to the binary attributes themselves, as the one-to-one pixel-level mapping of the noise latent to the generated image makes \(\mathbf{u}\) easily visualized. For example, we can clearly see how the blonde hair vector adds a yellow ‚Äòhalo‚Äô around the face which guides the DDIM to generate blonde hair, and how eyeglasses and smile have strongest effects around the eyes and the lips, respectively, which is sensible.</p> <p>Importantly, the visualization of the decision boundary provides insight into the dataset (CelebA), specifically the bias inherently within the dataset. For example, we notice that the blonde hair vector seems to assume that the hair of the generated face is long, which is an effect we see with the feature editing results where those with short hair seems to ‚Äògain‚Äô hair length when their hair becomes blonde. Similarly, the vectors for age and especially attractiveness display a rough face outline that looks more feminine than masculine, indicating not only an imbalance of these binary attributes between masculine and feminine faces (i.e., feminine faces are more likely to be labeled ‚Äúyoung‚Äù or ‚Äúattractive‚Äù). More broadly, since CelebA is a dataset of celebrities, we can argue that age and attractiveness are perhaps attributes more valued in female celebrities than male ones. We see this with the feature editing results as well, as age and attractiveness seems to be entangled with masculinity.</p> <p>This sort of visualization shows one advantage of the DDIM latent space over the GAN latent space‚Äîinterpretability. Even though the GAN latent space is arguably superior to the DDIM latent space in every way, with its lower dimensionality for easier computation, existence of more separated \(\mathcal{W}\) space with StyleGAN designs that provides coarse to fine control, and an inference process that requires just one forward propagation (and thus one backpropagation for most inversion techniques). However, the fact that the DDIM noise latent space is the same dimension as its image space, and that there is a direct spatial correspondence between the pixels in the noise latent space and that of the image space by the diffusion (denoising) process, means the noise latent space has great interpretability, as we have seen with the decision boundary visualization above.</p> <h2 id="conclusion">Conclusion</h2> <p>We explore implementing and training a DDIM from scratch, utilizing its deterministic sampling process to probe the relationship between its latent noise space \(\mathcal{Z}\) and the image space \(\mathcal{X}\) by applying existing GAN inversion techniques to the DDIM. Particularly, we find the following.</p> <ol> <li>Optimization-based inversion with the DDIM is possible and can produce relatively high-quality results, though currently it is limited to DDIM inferences with small <code class="language-plaintext highlighter-rouge">num_t_steps</code> as we must backpropagate through the model <code class="language-plaintext highlighter-rouge">num_t_steps</code> times.</li> <li>Learning-based (and consequently hybrid) inversion with the DDIM is currently difficult, both due to the high memory requirements with larger <code class="language-plaintext highlighter-rouge">num_t_steps</code> (which can cause gradient vanishing) and that \(\mathcal{Z}\) is high-dimensional like \(\mathcal{X}\) making encoders difficult to train.</li> <li>Interpolation between two noise latents leads to smooth visual interpolation in the image space for DDIMs, indicating that the latent space for diffusion models is smooth and meaningful, like that of GANs.</li> <li>We can perform semantic feature editing by finding decision boundaries of binary attributes of images in the noise latent and changing the noise latent by the normal of the hyperplane. Due to the matching dimensions and spatial correspondence of \(\mathcal{Z}\) and \(\mathcal{X}\), we can visualize the decision boundary like an image, demonstrating the unique interpretability of the DDIM latent space.</li> </ol> <p>The above shows the potential for research in manipulating the DDIM latent space directly, especially since the diffusion model can be conditioned in a variety of ways (e.g., class-condition, text-condition, and the recent ControlNet) in a variety of spaces (e.g., latent diffusion), which creates many latent spaces that can be manipulated in various ways.</p> <h3 id="future-work">Future Work</h3> <p>The following are some possible future directions based on the current project.</p> <ol> <li>The current implementation of our optimization-based method solely minimizes the error within the image space. However, we can also minimize the error within the feature space, i.e., try to match the VGG/Inception features of the target and reconstruction. Though doing so generally leads to images that deviate in detail to the original, it reduces overfitting of the trained noise latent and thus makes interpolation and semantic editing smoother.</li> <li>One of the biggest obstacles with learning-based (and consequently hybrid) inversion methods is the multiple forward (and thus backward) passes through the model. However, very recently OpenAI proposed consistency models, which, extremely simplified, is diffusion models that only require one forward pass [6]. In a way, the consistency model is similar to GANs with the generation process, but now it has the advantage of an interpretable latent space. It can be a potential direction for inversion.</li> <li>There, in fact, already exists a method, called EDICT (Exact Diffusion Inversion via Coupled Transformations) which modifies the inference process of the DDIM with coupled transformations that allows near-exact inversion with results that are genuinely impressive. They also showed how using the DDIM inference process on Stable Diffusion to find the image noise latent and text latent, then only modifying the text latent, allows for semantic editing targeting the subject [7]. EDICT can potentially make the semantic feature editing methods easier as they provide exact inversion on unseen images.</li> <li>The semantic face editing method is only a very small part of the bigger project, <a href="https://genforce.github.io/">GenForce</a>, which has a lot of GAN latent space manipulation methods (e.g., disentangling binary attributes via conditioning, better decision boundary computation with SVMs, better inversion methods, etc.) that can potentially be applied to DDIMs.</li> </ol> <h2 id="references">References</h2> <p>[1] Song, Jiaming, et al. ‚ÄúDenoising Diffusion Implicit Models.‚Äù <em>International Conference on Learning Representations</em>, 2021.</p> <p>[2] Xia, Weihao, et al. ‚ÄúGAN Inversion: A Survey.‚Äù <em>IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</em>, 2022.</p> <p>[3] Ho, Jonathan, et al. ‚ÄúDenoising Diffusion Probabilistic Models.‚Äù <em>Advances in Neural Information Processing Systems</em>, 2020, vol. 33, pp. 6840‚Äì6851.</p> <p>[4] Shen, Yujun, et al. ‚ÄúInterpreting the Latent Space of GANs for Semantic Face Editing.‚Äù <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2020.</p> <p>[5] He, Kaiming, et al. ‚ÄúDeep Residual Learning for Image Recognition.‚Äù <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2016.</p> <h3 id="future-work-references">Future Work References</h3> <p>[6] Song, Yang, et al. ‚ÄúConsistency Models.‚Äù <em>arXiv</em>:2303.01469, 2023.</p> <p>[7] Wallace, Bram, et al. ‚ÄúEDICT: Exact Diffusion Inversion via Coupled Transformations.‚Äù <em>arXiv</em>:2211.12446, 2022.</p> <h3 id="code-repository-references">Code Repository References</h3> <p>[1C] <a href="https://github.com/ermongroup/ddim">Original PyTorch implementation of DDIM</a> <br/> DDIM implementation from the original authors. I follow the code base closely for our implementation with slight changes, though I do not copy code and merely use their code as a vessel for understanding the paper. Particularly, the introduction section is entirely original by me and represents my ‚Äòtake‚Äô on DDPMs and DDIMs after having implemented and trained both from scratch. The organization of the code modules is also changed to better fit my preference for ML repositories, particularly integration with Jupyter Notebook so I can experiment with the trained models.</p> <p>[2C] <a href="https://keras.io/examples/generative/ddim/">Keras implementation of DDIM</a> <br/> Keras implementation of DDIM tested on Oxford Flowers102 dataset at a small scale trainable on my laptop. I took inspiration of some of their hyperparameter, criterion, and architecture design choices. Particularly, I inherited the use of \(L_1\) instead of \(L_2\) loss and the occasional batch normalization instead of group normalization for smaller networks and/or datasets with high detail.</p> <p>[3C] <a href="https://huggingface.co/blog/annotated-diffusion">The annotated diffusion model</a> <br/> The Hugging Face implementation and annotation of the DDPM. This is the original code I followed last quarter (when I was experimenting on my own) to get a basic understanding of DDPMs, and I partly based my code module organization and variable naming conventions on the blog post.</p>]]></content><author><name>Kuan Heng (Jordan) Lin</name></author><category term="research"/><category term="CV,"/><category term="generative_AI,"/><category term="diffusion"/><summary type="html"><![CDATA[We explore the inversion and latent space manipulation of diffusion models, particularly the denoising diffusion implicit model (DDIM), a variant of the denoising diffusion probabilistic model (DDPM) with deterministic (and acceleratable) sampling and thus a meaningful mapping from the latent space \(\mathcal{Z}\) to the image space \(\mathcal{X}\). We implement and compare optimization-based, learning-based, and hybrid inversion methods adapted from GAN inversion, and we find that optimization-based methods work well, but learning-based and hybrid methods run into obstacles fundamental to diffusion models. We also perform latent space interpolation to show that the DDIM latent space is continuous and meaningful, just like that of GANs. Lastly, we apply GAN semantic feature editing methods to DDIMs, visualizing binary attribute decision boundaries to showcase the unique interpretability of the diffusion latent space.]]></summary></entry><entry><title type="html">üì¢ KaoGPT: Studying the Performance of Text Generating Models</title><link href="https://kuanhenglin.github.io/blog/2023/kaogpt/" rel="alternate" type="text/html" title="üì¢ KaoGPT: Studying the Performance of Text Generating Models"/><published>2023-03-22T00:00:00+00:00</published><updated>2023-03-22T00:00:00+00:00</updated><id>https://kuanhenglin.github.io/blog/2023/kaogpt</id><content type="html" xml:base="https://kuanhenglin.github.io/blog/2023/kaogpt/"><![CDATA[<blockquote> <p>We developed text-generation models, including the RNN, decoder stack, encoder-decoder, and fine-tuned GPT-2, to emulate Professor Kao‚Äôs lectures. Through experimentation, we found that finetuning GPT-2 led to a model that outperformed all others. However, given the limited dataset, the trained-from-scratch decoder stack performed surprisingly well. Our results offer insights into the strengths and limitations of various text generation models, aiding researchers in selecting the most suitable model for their needs.</p> </blockquote> <iframe class="iframe-pdf" src="/assets/pdf/kaogpt.pdf" frameborder="0"></iframe> <p>This paper was written for the final project of UCLA‚Äôs ECE C147: Neural Networks and Deep Learning, Winter 2023, taught by Professor <a href="http://seas.ucla.edu/~kao/">Jonathan Kao</a>.</p>]]></content><author><name>Kuan Heng (Jordan) Lin</name></author><category term="research"/><category term="NLP,"/><category term="generative_AI,"/><category term="LLM"/><summary type="html"><![CDATA[We developed text-generation models, including the RNN, decoder stack, encoder-decoder, and fine-tuned GPT-2, to emulate Professor Kao‚Äôs lectures. Through experimentation, we found that finetuning GPT-2 led to a model that outperformed all others. However, given the limited dataset, the trained-from-scratch decoder stack performed surprisingly well. Our results offer insights into the strengths and limitations of various text generation models, aiding researchers in selecting the most suitable model for their needs.]]></summary></entry><entry><title type="html">üìí Wikisafe: HackMIT 2022 Blockchain for Society 2nd Prize</title><link href="https://kuanhenglin.github.io/blog/2022/wikisafe/" rel="alternate" type="text/html" title="üìí Wikisafe: HackMIT 2022 Blockchain for Society 2nd Prize"/><published>2022-10-02T00:00:00+00:00</published><updated>2022-10-02T00:00:00+00:00</updated><id>https://kuanhenglin.github.io/blog/2022/wikisafe</id><content type="html" xml:base="https://kuanhenglin.github.io/blog/2022/wikisafe/"><![CDATA[<blockquote> <p>Awarded <strong>Blockchain for Society 2nd Place</strong> (sponsored by Jump Crypto) as an entry for <a href="https://archive.hackmit.org/2022/"><strong>HackMIT 2022</strong></a>. We leveraged <code class="language-plaintext highlighter-rouge">Solidity</code> smart contracts on the Ethereum blockchain with <code class="language-plaintext highlighter-rouge">Web3.js</code> to achieve secure and attributable version management, along with deep generative models for summarization and caption &amp; illustration generation.</p> </blockquote> <div class="repo p-2 text-center github-repo-in-post"> <a href="https://github.com/bliutech/wikisafe" rel="external nofollow noopener" target="_blank"> <img class="repo-img-light w-100" alt="bliutech/wikisafe" src="https://github-readme-stats.vercel.app/api/pin/?username=bliutech&amp;repo=wikisafe&amp;theme=default&amp;show_owner=true"/> <img class="repo-img-dark w-100" alt="bliutech/wikisafe" src="https://github-readme-stats.vercel.app/api/pin/?username=bliutech&amp;repo=wikisafe&amp;theme=dark&amp;show_owner=true"/> </a> </div> <p><img src="/assets/img/wikisafe.png" width="100%"/></p> <p>Wikisafe is a revolutionary new crowdsourcing web application that innovates the process of crowdsourcing information. This application leverages the Ethereum blockchain to validate contributions to crowdsourced articles and prevents vandalism to these vital resources. Additionally, Wikisafe provides several fidelity machine learning models that seek to improve the overall contributor experience through automating parts of the process such as summarization, captioning, and figure generation. Contributors can start simply by signing up for an account and begin contributing and utilizing the features. Communities are able to collectively share knowledge and improve free and accessible education globally around the world with incredible ease, security, and quality!</p>]]></content><author><name>Jordan Lin</name></author><category term="project"/><category term="CV,"/><category term="NLP,"/><category term="blockchain,"/><category term="LLM,"/><category term="diffusion"/><summary type="html"><![CDATA[Awarded Blockchain for Society 2nd Place (sponsored by Jump Crypto) as an entry for HackMIT 2022. We leveraged Solidity smart contracts on the Ethereum blockchain with Web3.js to achieve secure and attributable version management, along with deep generative models for summarization and caption &amp; illustration generation.]]></summary></entry><entry><title type="html">ü•§ Lofi Beats to Scale and Rotate to</title><link href="https://kuanhenglin.github.io/blog/2022/fahrenheit-denialists/" rel="alternate" type="text/html" title="ü•§ Lofi Beats to Scale and Rotate to"/><published>2022-06-03T00:00:00+00:00</published><updated>2022-06-03T00:00:00+00:00</updated><id>https://kuanhenglin.github.io/blog/2022/fahrenheit-denialists</id><content type="html" xml:base="https://kuanhenglin.github.io/blog/2022/fahrenheit-denialists/"><![CDATA[<blockquote> <p>We created a 3D sandbox scene of the ‚Äúlofi beats to relax/study to‚Äù girl with a fully-fledged physics engine (with linear &amp; angular collision detection &amp; resolution), rigged models, and shadowing in <code class="language-plaintext highlighter-rouge">tiny-graphics.js</code>.</p> </blockquote> <div class="repo p-2 text-center github-repo-in-post"> <a href="https://github.com/kuanhenglin/fahrenheit-denialists" rel="external nofollow noopener" target="_blank"> <img class="repo-img-light w-100" alt="kuanhenglin/fahrenheit-denialists" src="https://github-readme-stats.vercel.app/api/pin/?username=kuanhenglin&amp;repo=fahrenheit-denialists&amp;theme=default&amp;show_owner=true"/> <img class="repo-img-dark w-100" alt="kuanhenglin/fahrenheit-denialists" src="https://github-readme-stats.vercel.app/api/pin/?username=kuanhenglin&amp;repo=fahrenheit-denialists&amp;theme=dark&amp;show_owner=true"/> </a> </div> <p><img src="/assets/img/fahrenheit_denialists/screenshot_scene_1.png" width="100%"/></p> <p>Our project is a 3D sandbox scene of the widely beloved study girl from the 24/7 Youtube radio ‚Äú<a href="https://youtu.be/5qap5aO4i9A">lofi hip hop radio - beats to relax/study to</a>.‚Äù When you start up the scene, you will see the study girl sitting in her chair just vibing to the chill beats. On her desk is a laptop, a notebook, and a study lamp. To her right is a window looking out to the night sky with a purring cat on the windowsill. We hope that our scene with give you the motivation to get though <em>finals</em>! Everything seems calm and peaceful until you unpause and unleash the underlying Physics engine that Jordan created.</p> <p><strong>Jordan Lin:</strong> Physics engine and shadowing <br/> <strong>Joice He:</strong> Modelling, art, texturing <br/> <strong>Aidan Cini:</strong> Scene setup</p> <h2 id="features">Features</h2> <p>The following is a list of features of our little demo.</p> <h3 id="toggle-pause">Toggle Pause</h3> <p>Initially the scene is static, meaning that time is completely stopped. Entering <code class="language-plaintext highlighter-rouge">Ctrl + p</code> will cause time to resume. You should see that the study girl and the table start vibrating, almost like they are vibing to the dope beats. This occurs because small impulses in the Physics engine can quickly snowball into larger impulses, which is a common problem with resting objects in Physics engines that do not dampen tiny motions‚Äîour engine!</p> <h3 id="toggle-bounding-boxes">Toggle Bounding Boxes</h3> <p><img src="/assets/img/fahrenheit_denialists/screenshot_bounding_1.png" width="100%"/></p> <p>Entering <code class="language-plaintext highlighter-rouge">Ctrl + b</code> turns on the bounding boxes of the objects, allowing the user to see how the objects collide. All bounding boxes are oriented bounding boxes ( rectangular prisms of arbitrary size and 3D rotation.) It is the most fun to watch the bounding boxes as the objects go flying around after unpausing.</p> <p>These bounding boxes act as the collision/hit boxes in our Physics engine.</p> <h3 id="toggle-blender">Toggle Blender</h3> <p><img src="/assets/img/fahrenheit_denialists/screenshot_blender_1.png" width="100%"/></p> <p>Entering <code class="language-plaintext highlighter-rouge">Ctrp + d</code> spawns a massive rectangular prism that spins around the floor and pushes everything in the scene around in the most chaotic way‚Äîwe call this a blender. The blender is not for the faint of heart; if you care about the study girl‚Äôs safety then please <strong>DO NOT USE</strong>.</p> <h3 id="shoot-object">Shoot Object</h3> <p>Entering <code class="language-plaintext highlighter-rouge">Ctrl + e</code> spawns ellipsoidal objects into the scene at roughly random positions and velocities (but all generally pointed towards the girl). When paused, this is not too exciting as the objects just stay static in the scene, but unpause and you can watch all the new objects fly around the scene.</p> <h3 id="angular-velocity-and-impulse">Angular Velocity and Impulse</h3> <p><img src="/assets/img/fahrenheit_denialists/screenshot_angular_1.png" width="100%"/></p> <p>When angular velocity and impulse is enabled with <code class="language-plaintext highlighter-rouge">Ctrl + 6</code>, the Physics engine enables angular collision resolution and collisions now affect the rotation and angular velocity of objects. Now, this is not particularly stable, because gravity currently only works on the center-of-mass of objects/object groups instead of all parts of the object. Something something snowballing, and chaos ensues.</p> <p>When angular velocity and impulse is disabled with <code class="language-plaintext highlighter-rouge">Ctrl + 7</code>, the Physics engine disables angular collision resolution. This is the <em>default</em> of our demo and is a bit more stable, though still not terribly stable. Stay paused if you want peace, though that is not at all the objective of this demo.</p> <h3 id="initialize-scene">Initialize Scene</h3> <p>Entering <code class="language-plaintext highlighter-rouge">Ctrl + i</code> reinitializes the scene to its original state, which is useful for clearing objects. If the scene and Physics engine crashes (mostly due to spawning too many objects), the initialize scene button will not work, and you will need to refresh the webpage.</p> <h3 id="ungroup-objects">Ungroup objects</h3> <table> <thead> <tr> <th style="text-align: center">Without Hitboxes</th> <th style="text-align: center">With Hitboxes</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><img src="/assets/img/fahrenheit_denialists/screenshot_ungroup_1.png" width="100%"/></td> <td style="text-align: center"><img src="/assets/img/fahrenheit_denialists/screenshot_ungroup_2.png" width="100%"/></td> </tr> </tbody> </table> <p>Do you want to see the study girl break into pieces? If so, this is the feature for you. Since the study girl is a very complicated object to model with complex collision boxes, we modeled her body by breaking it into many different simpler shapes. This button ungroups the individual components of grouped objects (e.g., study girl, desk, chair, etc.) and allows them to move and collide freely.</p> <h3 id="gravity">Gravity</h3> <p><img src="/assets/img/fahrenheit_denialists/screenshot_gravity_1.png" width="100%"/></p> <p>You can change gravity to any one of the coordinate axes. Simply click the button that displays the direction you want gravity to go and it will follow. <code class="language-plaintext highlighter-rouge">none</code> causes the objects to float about, <code class="language-plaintext highlighter-rouge">+x</code> makes the objects fly backward, <code class="language-plaintext highlighter-rouge">-x</code> pushes the objects toward the brick wall, <code class="language-plaintext highlighter-rouge">+y</code> makes the objects fly to the ceiling, <code class="language-plaintext highlighter-rouge">-y</code> is normal gravity pushing objects to the floor, <code class="language-plaintext highlighter-rouge">+z</code> makes the objects come toward the camera, and <code class="language-plaintext highlighter-rouge">-z</code> makes the objects fly towards the window wall.</p> <h3 id="movement">Movement</h3> <p><img src="/assets/img/fahrenheit_denialists/screenshot_scene_2.png" width="100%"/></p> <p>As per a feature which comes with <code class="language-plaintext highlighter-rouge">tiny-graphics.js</code>, you can move around the scene with <code class="language-plaintext highlighter-rouge">w</code>, <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">s</code> and <code class="language-plaintext highlighter-rouge">d</code>; you can also move up with <code class="language-plaintext highlighter-rouge">Space</code> and move down with <code class="language-plaintext highlighter-rouge">z</code>.</p> <h2 id="running-the-code">Running the Code</h2> <p>Clone this <a href="https://github.com/kuanhenglin/fahrenheit-denialists">repository</a> to your local machine.</p> <p>For Windows: run/double-click the <code class="language-plaintext highlighter-rouge">host.bat</code> file in the main directory. <br/> For MacOS: run the <code class="language-plaintext highlighter-rouge">host.command</code> file. Alternatively, execute <code class="language-plaintext highlighter-rouge">python3 server.py</code> or <code class="language-plaintext highlighter-rouge">python3 server.py</code> (should work for both Windows and MacOS). <br/> If you are using Linux, you can probably figure this out yourself.</p> <p>Then, type <code class="language-plaintext highlighter-rouge">localhost:8000</code> into your browser, hit enter and have fun :D</p> <p>We use <a href="https://github.com/encyclopedia-of-code/tiny-graphics-js.git"><code class="language-plaintext highlighter-rouge">tiny-graphics.js</code></a>, which is like <a href="https://threejs.org/"><code class="language-plaintext highlighter-rouge">three.js</code></a> but worse.</p> <p><em>Have fun exploring our demo!</em></p> <hr/> <p>This is our final project for UCLA‚Äôs CS 174A: Introduction to Computer Graphics, Spring 2022, taught by <a href="https://www.linkedin.com/in/asishlaw/">Dr. Asish Law</a>.</p>]]></content><author><name>Jordan Lin</name></author><category term="project"/><category term="graphics,"/><category term="physics_engine"/><summary type="html"><![CDATA[We created a 3D sandbox scene of the ‚Äúlofi beats to relax/study to‚Äù girl with a fully-fledged physics engine (with linear &amp; angular collision detection &amp; resolution), rigged models, and shadowing in tiny-graphics.js.]]></summary></entry><entry><title type="html">ü•ö T-Eggletop Map Creator</title><link href="https://kuanhenglin.github.io/blog/2022/eggmacs/" rel="alternate" type="text/html" title="ü•ö T-Eggletop Map Creator"/><published>2022-03-11T00:00:00+00:00</published><updated>2022-03-11T00:00:00+00:00</updated><id>https://kuanhenglin.github.io/blog/2022/eggmacs</id><content type="html" xml:base="https://kuanhenglin.github.io/blog/2022/eggmacs/"><![CDATA[<blockquote> <p>We created a homebrew tabletop map designer based on the MERN framework that is intended to make the campaign design process faster and easier for dungeon masters of all levels, including us :D</p> </blockquote> <div class="repo p-2 text-center github-repo-in-post"> <a href="https://github.com/kuanhenglin/eggmacs" rel="external nofollow noopener" target="_blank"> <img class="repo-img-light w-100" alt="kuanhenglin/eggmacs" src="https://github-readme-stats.vercel.app/api/pin/?username=kuanhenglin&amp;repo=eggmacs&amp;theme=default&amp;show_owner=false"/> <img class="repo-img-dark w-100" alt="kuanhenglin/eggmacs" src="https://github-readme-stats.vercel.app/api/pin/?username=kuanhenglin&amp;repo=eggmacs&amp;theme=dark&amp;show_owner=false"/> </a> </div> <p><img src="/assets/img/eggmacs/cover.png" width="100%"/></p> <p>The <strong>T-Eggletop Map Creator (TeMC)</strong> is a homebrew tabletop map designer intended to make the campaign design process faster and easier, especially for GMs who are not artistically inclined. TeMC is a web application that operates using the MERN framework, (MongoDB, Expression.js, React.js, Node.js.) Both the frontend and backend are hosted on Heroku.</p> <p>We on Team <code class="language-plaintext highlighter-rouge">Eggmacs</code> know firsthand how painful it is to lug around folders of graph paper to all your D&amp;D sessions. Using our web application, users are able to draft, create, save, and download their own comprehensive maps in minutes, as well as browse maps published by other users on the site.</p> <p><strong>Joice:</strong> CSS, art &amp; design, map item display <strong>Jordan:</strong> Full-stack, user authentication, database API, map structure <strong>Kay:</strong> Backend, search API, map download <strong>Meghana:</strong> Map creator, interface <strong>Sean:</strong> Operations, deployment, search/profile features</p> <h2 id="features">Features</h2> <h3 id="map-creator">Map Creator</h3> <p><img class="margin-center" src="/assets/img/eggmacs/page_creator.png" width="100%"/></p> <p>The map creator is our central feature. Users can select tiles, assets, and characters from a bar and design their maps in the grid space provided. Tiles are the background, assets add the details, and characters are the avatars.</p> <p>The item bar is organized into tiles, assets, and characters. Clicking on an item in the item bar will put the user in a ‚Äúplacement mode‚Äù where all clicks will place that item in the grid. Hovering over a grid box will highlight it, indicating the spot where the asset will be placed. The grid is toggleable so that users can see what the downloaded png will look like, or edit without the grid if they prefer.</p> <p>This map is implemented using four layers: two display layers and two placement layers (one for tile sized items and one for asset sized items.) The display layers are displayed on top of each other to create the full map display.</p> <h3 id="search--map-cataloguing">Search &amp; Map Cataloguing</h3> <p><img class="margin-center" src="/assets/img/eggmacs/page_search.png" width="80%"/></p> <p>There is a search feature where users are able to search through other users and maps created. The search is done by assigning different objects with score rankings using substrings, then displaying results in a listed format. Every entry is hyperlinked so you can navigate directly to the user profile or map from the search results.</p> <p>Users are able to search for and open maps from the search page or user profiles. This loads the map into the map creator. Created maps can be saved and become searchable by other users.</p> <p>Saving a map (via a button click) creates a database object containing the map‚Äôs unique ID, tiles, and assets. Loading a map pulls that information from the database and brings it up in the route associated with that map ID.</p> <h3 id="map-download">Map Download</h3> <p>Downloading the map as a png is our third unique feature. Users can export their finished maps as a png with a transparent background. Given the map object, the download algorithm uses an HTML5 Canvas with the dimensions of the map, iterates through the arrays of items, grabs the images, and converts them into File objects that can be drawn onto the canvas. The canvas object is then converted into a downloaded png that pops up in the browser.</p> <h3 id="user-authentication-profiles-and-personalized-views">User Authentication, Profiles, and Personalized Views</h3> <p><img class="margin-center" src="/assets/img/eggmacs/page_profile.png" width="100%"/></p> <p>The profile and map creation pages are customized based on who is logged in and which map is being viewed. Profiles are fetched using UseEffect at page load. The options on the map creator are also dynamically displayed based on whether or not the user is the author of the map. If they are, it gives them delete/save functions, and if they aren‚Äôt, they are only allowed to make one time changes and download their modified version.</p> <p><img class="margin-center" src="/assets/img/eggmacs/page_admin.png" width="100%"/></p> <p>Creating an account and modifying a profile is instant. At sign up, a new user object is created, and at sign in, the user info is verified with a fetched user object. The avatar is stored on our MongoDB server as a base64 bytestring. Every update or upload is sent from front to back as an object, then the backend sends the new object to MongoDB via the Node driver.</p> <h2 id="example-maps">Example Maps</h2> <p>Here are some example maps from our users!</p> <p><img class="margin-center" src="/assets/img/eggmacs/map_4Goose.png" width="100%"/> <em>‚ÄúSus‚Äù by <code class="language-plaintext highlighter-rouge">4Goose</code></em></p> <p><img class="margin-center" src="/assets/img/eggmacs/map_gowin.png" width="100%"/> <em>‚Äúmap of UCLA Rooftops‚Äù by <code class="language-plaintext highlighter-rouge">gowin</code></em></p> <p><img class="margin-center" src="/assets/img/eggmacs/map_joice.png" width="100%"/> <em>‚ÄúA MURDER‚Äù by <code class="language-plaintext highlighter-rouge">theresonlyjuice</code></em></p> <p><img class="margin-center" src="/assets/img/eggmacs/map_kay.png" width="100%"/> <em>‚ÄúDesertOasis1‚Äù by <code class="language-plaintext highlighter-rouge">the-bay-kay</code></em></p> <p><img class="margin-center" src="/assets/img/eggmacs/map_sean.png" width="100%"/> <em>‚ÄúFranz Hall 1178‚Äù by <code class="language-plaintext highlighter-rouge">Shalphan</code></em></p> <hr/> <p>This is our final project for UCLA‚Äôs CS 35L: Software Construction, Winter 2022, taught by Professor <a href="https://samueli.ucla.edu/people/paul-eggert/">Paul <strong>Egg</strong>ert</a>.</p>]]></content><author><name>Jordan Lin</name></author><category term="project"/><category term="SWE,"/><category term="D&amp;D,"/><category term="full-stack"/><summary type="html"><![CDATA[We created a homebrew tabletop map designer based on the MERN framework that is intended to make the campaign design process faster and easier for dungeon masters of all levels, including us :D]]></summary></entry></feed>